{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.474008100Z",
     "start_time": "2023-05-20T08:52:06.012082500Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.504903800Z",
     "start_time": "2023-05-20T08:52:08.476031400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Settings(BaseModel):\n",
    "    project_name: str = 'jp50_02'\n",
    "    device: str = 'cuda'\n",
    "    epoch: int = 200\n",
    "    batch: int = 8\n",
    "    learning_rate: float = 2e-4\n",
    "    image_size: int = 64\n",
    "    sample_interval: int = 500\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 16\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 64\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "    # Establish convention for real and fake labels during training\n",
    "    real_label = 1.\n",
    "    fake_label = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.504903800Z",
     "start_time": "2023-05-20T08:52:08.482647500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf,kernel_size= 4,stride= 2,padding= 1, bias=False),#(64-4+2)/2=31+1=32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),#(32-4+2)/2=31+1=16\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),#(16-4+2)/2=8\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),#(8-4+2)/2  +1=4\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),# 4-4\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.504903800Z",
     "start_time": "2023-05-20T08:52:08.487126400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset (Data Loader)\n",
    "\n",
    "- Here use pytorch built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.504903800Z",
     "start_time": "2023-05-20T08:52:08.492430300Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import mnist\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 調整大小為 32x32\n",
    "    transforms.Grayscale(num_output_channels=1),  # 轉為單通道灰度圖像\n",
    "    transforms.ToTensor(),  # 轉為張量\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 歸一化\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:52:08.517159300Z",
     "start_time": "2023-05-20T08:52:08.494930600Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self) -> None:\n",
    "        self.args = Settings()\n",
    "\n",
    "        wandb.init(project=self.args.project_name, config=self.args.dict(), save_code=True)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.dataset = datasets.ImageFolder(root='./dataset/train_images', transform=transform) # datasets.ImageFolder 會將資料夾中的圖片依照資料夾名稱分類\n",
    "        #訓練集\n",
    "        self.loader = DataLoader(self.dataset, batch_size=self.args.batch, shuffle=True)  #每次訓練數量 = Data set size(*0.8) / Batch size = 800/20 = 40\n",
    "\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        self.g_model, self.g_loss, self.g_optim = self.generator()\n",
    "        self.d_model, self.d_loss, self.d_optim = self.discriminator()\n",
    "\n",
    "    def generator(self):\n",
    "        model = Generator(nc=self.args.nc, ngf=self.args.ngf, nz=self.args.nz).to(self.args.device)\n",
    "        model.apply(self.weights_init)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args.learning_rate, betas=(self.args.beta1, 0.999))\n",
    "        return model, criterion, optimizer\n",
    "\n",
    "    def discriminator(self):\n",
    "        model = Discriminator(nc=self.args.nc, ndf=self.args.ndf).to(self.args.device)\n",
    "        model.apply(self.weights_init)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args.learning_rate, betas=(self.args.beta1, 0.999))\n",
    "        return model, criterion, optimizer\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        # custom weights initialization called on netG and netD\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def train_one_epoch(self, i_epoch: int):\n",
    "        self.metric = {\n",
    "            'train_d': {},\n",
    "            'train_g': {},\n",
    "        }\n",
    "        self.i_epoch = i_epoch\n",
    "        self.d_model.train(mode=True)\n",
    "        self.g_model.train(mode=True)\n",
    "        bar = tqdm(self.loader, unit='batch', leave=True)\n",
    "        for i_batch, (data, label) in enumerate(bar):\n",
    "            self.step = (i_epoch * len(self.loader) + i_batch)\n",
    "            data = data.to(self.args.device)\n",
    "\n",
    "            fake = self.train_d(data)\n",
    "\n",
    "            self.train_g(fake)\n",
    "\n",
    "            self.show_result(fake)\n",
    "\n",
    "            loss = {\n",
    "                'd_loss': self.metric['train_d']['loss'][-1],\n",
    "                'g_loss': self.metric['train_g']['loss'][-1],\n",
    "            }\n",
    "            wandb.log({**loss, **{'i_epoch': self.i_epoch, 'step': self.step, }}, step=self.step)\n",
    "\n",
    "            bar.set_description(f'Epoch [{self.i_epoch + 1}/{self.args.epoch}]')\n",
    "            bar.set_postfix(**loss)\n",
    "        return 0\n",
    "\n",
    "    def show_result(self, fake):\n",
    "        if self.step % self.args.sample_interval == 0:\n",
    "            wandb.log({\n",
    "                'fake': [wandb.Image(im.permute(1,2,0).detach().cpu().numpy()) for index, im in enumerate(fake) if index < 24]\n",
    "            }, step=self.step)\n",
    "        return 0\n",
    "\n",
    "    def train_d(self, data):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # Train with all-real batch\n",
    "        self.d_optim.zero_grad()\n",
    "        # Format batch\n",
    "        real_inputs = data.to(self.args.device)\n",
    "        b_size = real_inputs.size(0)\n",
    "        label = torch.full((b_size,), self.args.real_label, dtype=torch.float, device=self.args.device)\n",
    "        # Forward pass real batch through D\n",
    "        output = self.d_model.forward(real_inputs).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = self.d_loss.forward(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        torch.autograd.backward(errD_real)\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, self.args.nz, 1, 1, device=self.args.device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = self.g_model.forward(noise)\n",
    "        label.fill_(self.args.fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = self.d_model.forward(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = self.d_loss.forward(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        torch.autograd.backward(errD_fake)\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        self.d_optim.step()\n",
    "        self.metric['train_d'].setdefault('loss', []).append(errD.item())\n",
    "        return fake\n",
    "\n",
    "    def train_g(self, fake):\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        self.g_optim.zero_grad()\n",
    "        # fake labels are real for generator cost\n",
    "        label = torch.full((fake.size(0),), self.args.real_label, dtype=torch.float, device=self.args.device)\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = self.d_model.forward(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = self.d_loss(output, label)\n",
    "        # Calculate gradients for G\n",
    "        torch.autograd.backward(errG)\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        self.g_optim.step()\n",
    "        self.metric['train_g'].setdefault('loss', []).append(errG.item())\n",
    "        return 0\n",
    "\n",
    "    def train(self):\n",
    "        for i_epoch in range(self.args.epoch):\n",
    "            self.train_one_epoch(i_epoch)\n",
    "            self.validation()\n",
    "            self.save_model()\n",
    "        return 0\n",
    "\n",
    "    def validation(self):\n",
    "        return 0\n",
    "\n",
    "    def test(self):\n",
    "        return 0\n",
    "\n",
    "    def save_model(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T08:59:06.373658900Z",
     "start_time": "2023-05-20T08:52:08.511175200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhj6hki123\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>F:\\CODING\\Pytorch_Learning\\wandb\\run-20230520_165210-8tpmzeo1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hj6hki123/jp50_02/runs/8tpmzeo1' target=\"_blank\">eternal-plant-1</a></strong> to <a href='https://wandb.ai/hj6hki123/jp50_02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hj6hki123/jp50_02' target=\"_blank\">https://wandb.ai/hj6hki123/jp50_02</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hj6hki123/jp50_02/runs/8tpmzeo1' target=\"_blank\">https://wandb.ai/hj6hki123/jp50_02/runs/8tpmzeo1</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]: 100%|██████████| 125/125 [00:02<00:00, 43.53batch/s, d_loss=0.0555, g_loss=9.67] \n",
      "Epoch [2/200]: 100%|██████████| 125/125 [00:02<00:00, 61.11batch/s, d_loss=0.284, g_loss=15.4]   \n",
      "Epoch [3/200]: 100%|██████████| 125/125 [00:02<00:00, 58.03batch/s, d_loss=0.2, g_loss=14]      \n",
      "Epoch [4/200]: 100%|██████████| 125/125 [00:02<00:00, 61.91batch/s, d_loss=0.0118, g_loss=6.94]  \n",
      "Epoch [5/200]: 100%|██████████| 125/125 [00:02<00:00, 59.80batch/s, d_loss=0.074, g_loss=4.66]  \n",
      "Epoch [6/200]: 100%|██████████| 125/125 [00:01<00:00, 63.09batch/s, d_loss=0.0463, g_loss=13.5]  \n",
      "Epoch [7/200]: 100%|██████████| 125/125 [00:01<00:00, 63.03batch/s, d_loss=0.0163, g_loss=7.02] \n",
      "Epoch [8/200]: 100%|██████████| 125/125 [00:02<00:00, 61.68batch/s, d_loss=0.00409, g_loss=6.8]  \n",
      "Epoch [9/200]: 100%|██████████| 125/125 [00:02<00:00, 58.32batch/s, d_loss=0.0628, g_loss=4.82]  \n",
      "Epoch [10/200]: 100%|██████████| 125/125 [00:02<00:00, 58.43batch/s, d_loss=0.004, g_loss=7.2]   \n",
      "Epoch [11/200]: 100%|██████████| 125/125 [00:02<00:00, 56.43batch/s, d_loss=0.0883, g_loss=7.12] \n",
      "Epoch [12/200]: 100%|██████████| 125/125 [00:02<00:00, 58.35batch/s, d_loss=0.00259, g_loss=7.83]\n",
      "Epoch [13/200]: 100%|██████████| 125/125 [00:02<00:00, 59.37batch/s, d_loss=0.123, g_loss=7.18]  \n",
      "Epoch [14/200]: 100%|██████████| 125/125 [00:01<00:00, 62.70batch/s, d_loss=0.00609, g_loss=7.97]\n",
      "Epoch [15/200]: 100%|██████████| 125/125 [00:01<00:00, 63.33batch/s, d_loss=0.0809, g_loss=6.18]  \n",
      "Epoch [16/200]: 100%|██████████| 125/125 [00:02<00:00, 61.40batch/s, d_loss=0.00151, g_loss=8.07]\n",
      "Epoch [17/200]: 100%|██████████| 125/125 [00:02<00:00, 60.15batch/s, d_loss=0.0426, g_loss=8.56] \n",
      "Epoch [18/200]: 100%|██████████| 125/125 [00:01<00:00, 63.40batch/s, d_loss=0.00775, g_loss=7.92]\n",
      "Epoch [19/200]: 100%|██████████| 125/125 [00:01<00:00, 63.42batch/s, d_loss=0.019, g_loss=8.91]   \n",
      "Epoch [20/200]: 100%|██████████| 125/125 [00:01<00:00, 63.56batch/s, d_loss=0.00161, g_loss=8.24]\n",
      "Epoch [21/200]: 100%|██████████| 125/125 [00:02<00:00, 59.62batch/s, d_loss=0.356, g_loss=6.68]   \n",
      "Epoch [22/200]: 100%|██████████| 125/125 [00:02<00:00, 61.86batch/s, d_loss=0.0475, g_loss=5.07] \n",
      "Epoch [23/200]: 100%|██████████| 125/125 [00:01<00:00, 62.98batch/s, d_loss=0.117, g_loss=8.13]  \n",
      "Epoch [24/200]: 100%|██████████| 125/125 [00:01<00:00, 63.28batch/s, d_loss=0.0218, g_loss=3.92] \n",
      "Epoch [25/200]: 100%|██████████| 125/125 [00:02<00:00, 59.75batch/s, d_loss=0.00483, g_loss=8.12]\n",
      "Epoch [26/200]: 100%|██████████| 125/125 [00:02<00:00, 61.53batch/s, d_loss=0.55, g_loss=6.06]  \n",
      "Epoch [27/200]: 100%|██████████| 125/125 [00:02<00:00, 60.76batch/s, d_loss=0.36, g_loss=7.81]  \n",
      "Epoch [28/200]: 100%|██████████| 125/125 [00:01<00:00, 62.95batch/s, d_loss=0.0865, g_loss=6.21]\n",
      "Epoch [29/200]: 100%|██████████| 125/125 [00:02<00:00, 61.15batch/s, d_loss=0.856, g_loss=9.13] \n",
      "Epoch [30/200]: 100%|██████████| 125/125 [00:01<00:00, 63.10batch/s, d_loss=0.718, g_loss=3.92] \n",
      "Epoch [31/200]: 100%|██████████| 125/125 [00:02<00:00, 61.90batch/s, d_loss=0.021, g_loss=6.57]  \n",
      "Epoch [32/200]: 100%|██████████| 125/125 [00:02<00:00, 60.65batch/s, d_loss=0.0282, g_loss=3.99]\n",
      "Epoch [33/200]: 100%|██████████| 125/125 [00:02<00:00, 60.54batch/s, d_loss=0.105, g_loss=5.1]   \n",
      "Epoch [34/200]: 100%|██████████| 125/125 [00:01<00:00, 63.54batch/s, d_loss=0.0769, g_loss=4.12] \n",
      "Epoch [35/200]: 100%|██████████| 125/125 [00:01<00:00, 63.48batch/s, d_loss=0.0519, g_loss=4.09] \n",
      "Epoch [36/200]: 100%|██████████| 125/125 [00:01<00:00, 62.79batch/s, d_loss=0.00337, g_loss=9.19]\n",
      "Epoch [37/200]: 100%|██████████| 125/125 [00:02<00:00, 58.19batch/s, d_loss=0.0819, g_loss=4.96] \n",
      "Epoch [38/200]: 100%|██████████| 125/125 [00:01<00:00, 62.90batch/s, d_loss=1.11, g_loss=3.24]  \n",
      "Epoch [39/200]: 100%|██████████| 125/125 [00:02<00:00, 62.32batch/s, d_loss=0.00361, g_loss=7.6]\n",
      "Epoch [40/200]: 100%|██████████| 125/125 [00:01<00:00, 63.14batch/s, d_loss=0.049, g_loss=4.92]  \n",
      "Epoch [41/200]: 100%|██████████| 125/125 [00:02<00:00, 61.58batch/s, d_loss=0.0254, g_loss=4.34] \n",
      "Epoch [42/200]: 100%|██████████| 125/125 [00:02<00:00, 60.41batch/s, d_loss=0.623, g_loss=3.02]  \n",
      "Epoch [43/200]: 100%|██████████| 125/125 [00:02<00:00, 62.34batch/s, d_loss=0.0272, g_loss=4.75]\n",
      "Epoch [44/200]: 100%|██████████| 125/125 [00:01<00:00, 63.38batch/s, d_loss=0.0115, g_loss=7.57] \n",
      "Epoch [45/200]: 100%|██████████| 125/125 [00:02<00:00, 61.68batch/s, d_loss=0.0103, g_loss=8.13] \n",
      "Epoch [46/200]: 100%|██████████| 125/125 [00:01<00:00, 63.51batch/s, d_loss=0.0893, g_loss=11.2]  \n",
      "Epoch [47/200]: 100%|██████████| 125/125 [00:02<00:00, 61.23batch/s, d_loss=0.139, g_loss=4.55]  \n",
      "Epoch [48/200]: 100%|██████████| 125/125 [00:02<00:00, 62.21batch/s, d_loss=0.221, g_loss=4.87]  \n",
      "Epoch [49/200]: 100%|██████████| 125/125 [00:02<00:00, 61.35batch/s, d_loss=0.00335, g_loss=8.45]\n",
      "Epoch [50/200]: 100%|██████████| 125/125 [00:01<00:00, 63.48batch/s, d_loss=0.0108, g_loss=7.79] \n",
      "Epoch [51/200]: 100%|██████████| 125/125 [00:01<00:00, 62.75batch/s, d_loss=0.292, g_loss=5.54]  \n",
      "Epoch [52/200]: 100%|██████████| 125/125 [00:02<00:00, 60.79batch/s, d_loss=0.026, g_loss=6.82] \n",
      "Epoch [53/200]: 100%|██████████| 125/125 [00:02<00:00, 59.55batch/s, d_loss=0.0485, g_loss=6.55] \n",
      "Epoch [54/200]: 100%|██████████| 125/125 [00:01<00:00, 63.31batch/s, d_loss=0.00466, g_loss=7.33] \n",
      "Epoch [55/200]: 100%|██████████| 125/125 [00:01<00:00, 63.61batch/s, d_loss=0.00454, g_loss=9.48] \n",
      "Epoch [56/200]: 100%|██████████| 125/125 [00:01<00:00, 63.55batch/s, d_loss=0.00143, g_loss=9.43] \n",
      "Epoch [57/200]: 100%|██████████| 125/125 [00:02<00:00, 59.96batch/s, d_loss=0.000975, g_loss=10.1]\n",
      "Epoch [58/200]: 100%|██████████| 125/125 [00:02<00:00, 61.31batch/s, d_loss=2.75, g_loss=7.36]   \n",
      "Epoch [59/200]: 100%|██████████| 125/125 [00:01<00:00, 63.52batch/s, d_loss=0.178, g_loss=5.08] \n",
      "Epoch [60/200]: 100%|██████████| 125/125 [00:01<00:00, 63.32batch/s, d_loss=0.0776, g_loss=6.26]\n",
      "Epoch [61/200]: 100%|██████████| 125/125 [00:02<00:00, 61.15batch/s, d_loss=0.00727, g_loss=6.85]\n",
      "Epoch [62/200]: 100%|██████████| 125/125 [00:02<00:00, 62.24batch/s, d_loss=0.44, g_loss=6.76]  \n",
      "Epoch [63/200]: 100%|██████████| 125/125 [00:02<00:00, 60.80batch/s, d_loss=0.0366, g_loss=5.97]\n",
      "Epoch [64/200]: 100%|██████████| 125/125 [00:01<00:00, 63.16batch/s, d_loss=0.0328, g_loss=7.24] \n",
      "Epoch [65/200]: 100%|██████████| 125/125 [00:02<00:00, 61.77batch/s, d_loss=0.0071, g_loss=6.89] \n",
      "Epoch [66/200]: 100%|██████████| 125/125 [00:01<00:00, 62.77batch/s, d_loss=0.00239, g_loss=8.14]\n",
      "Epoch [67/200]: 100%|██████████| 125/125 [00:02<00:00, 62.37batch/s, d_loss=0.018, g_loss=7.13]   \n",
      "Epoch [68/200]: 100%|██████████| 125/125 [00:02<00:00, 59.95batch/s, d_loss=0.029, g_loss=6.82]   \n",
      "Epoch [69/200]: 100%|██████████| 125/125 [00:02<00:00, 60.41batch/s, d_loss=0.000633, g_loss=8.85]\n",
      "Epoch [70/200]: 100%|██████████| 125/125 [00:01<00:00, 62.98batch/s, d_loss=0.0161, g_loss=8.36]  \n",
      "Epoch [71/200]: 100%|██████████| 125/125 [00:01<00:00, 63.21batch/s, d_loss=0.00101, g_loss=8.82] \n",
      "Epoch [72/200]: 100%|██████████| 125/125 [00:01<00:00, 62.80batch/s, d_loss=1.12, g_loss=4.61]   \n",
      "Epoch [73/200]: 100%|██████████| 125/125 [00:02<00:00, 57.80batch/s, d_loss=0.0726, g_loss=4.9] \n",
      "Epoch [74/200]: 100%|██████████| 125/125 [00:02<00:00, 61.04batch/s, d_loss=0.0771, g_loss=5.81]\n",
      "Epoch [75/200]: 100%|██████████| 125/125 [00:01<00:00, 62.54batch/s, d_loss=0.129, g_loss=4.84]  \n",
      "Epoch [76/200]: 100%|██████████| 125/125 [00:01<00:00, 62.54batch/s, d_loss=0.0409, g_loss=7.3]  \n",
      "Epoch [77/200]: 100%|██████████| 125/125 [00:02<00:00, 59.20batch/s, d_loss=0.0812, g_loss=7.58]\n",
      "Epoch [78/200]: 100%|██████████| 125/125 [00:02<00:00, 60.31batch/s, d_loss=0.0302, g_loss=6.26] \n",
      "Epoch [79/200]: 100%|██████████| 125/125 [00:01<00:00, 62.83batch/s, d_loss=0.0347, g_loss=6.67] \n",
      "Epoch [80/200]: 100%|██████████| 125/125 [00:01<00:00, 63.62batch/s, d_loss=0.00506, g_loss=8.02]\n",
      "Epoch [81/200]: 100%|██████████| 125/125 [00:02<00:00, 61.59batch/s, d_loss=0.000783, g_loss=8.65]\n",
      "Epoch [82/200]: 100%|██████████| 125/125 [00:01<00:00, 63.37batch/s, d_loss=0.337, g_loss=2.27]  \n",
      "Epoch [83/200]: 100%|██████████| 125/125 [00:02<00:00, 61.75batch/s, d_loss=0.566, g_loss=3.73] \n",
      "Epoch [84/200]: 100%|██████████| 125/125 [00:02<00:00, 62.00batch/s, d_loss=0.0324, g_loss=5.89] \n",
      "Epoch [85/200]: 100%|██████████| 125/125 [00:02<00:00, 61.28batch/s, d_loss=0.037, g_loss=9.36]  \n",
      "Epoch [86/200]: 100%|██████████| 125/125 [00:01<00:00, 62.59batch/s, d_loss=0.00352, g_loss=9.66] \n",
      "Epoch [87/200]: 100%|██████████| 125/125 [00:01<00:00, 62.69batch/s, d_loss=0.0015, g_loss=10]    \n",
      "Epoch [88/200]: 100%|██████████| 125/125 [00:02<00:00, 61.70batch/s, d_loss=0.119, g_loss=5.93]  \n",
      "Epoch [89/200]: 100%|██████████| 125/125 [00:02<00:00, 60.02batch/s, d_loss=0.00423, g_loss=8.11]\n",
      "Epoch [90/200]: 100%|██████████| 125/125 [00:01<00:00, 63.73batch/s, d_loss=0.00452, g_loss=6.79] \n",
      "Epoch [91/200]: 100%|██████████| 125/125 [00:01<00:00, 63.67batch/s, d_loss=0.0168, g_loss=10.1]  \n",
      "Epoch [92/200]: 100%|██████████| 125/125 [00:01<00:00, 63.44batch/s, d_loss=0.465, g_loss=5.57]  \n",
      "Epoch [93/200]: 100%|██████████| 125/125 [00:02<00:00, 59.05batch/s, d_loss=0.0723, g_loss=7.59] \n",
      "Epoch [94/200]: 100%|██████████| 125/125 [00:02<00:00, 59.84batch/s, d_loss=0.00234, g_loss=8.56]\n",
      "Epoch [95/200]: 100%|██████████| 125/125 [00:02<00:00, 60.44batch/s, d_loss=0.019, g_loss=7.02]   \n",
      "Epoch [96/200]: 100%|██████████| 125/125 [00:02<00:00, 62.47batch/s, d_loss=0.00957, g_loss=7.88] \n",
      "Epoch [97/200]: 100%|██████████| 125/125 [00:02<00:00, 53.10batch/s, d_loss=0.0248, g_loss=7.93] \n",
      "Epoch [98/200]: 100%|██████████| 125/125 [00:02<00:00, 61.22batch/s, d_loss=0.00176, g_loss=9.28]\n",
      "Epoch [99/200]: 100%|██████████| 125/125 [00:02<00:00, 60.48batch/s, d_loss=0.0441, g_loss=5.48] \n",
      "Epoch [100/200]: 100%|██████████| 125/125 [00:01<00:00, 63.32batch/s, d_loss=0.055, g_loss=6.7]    \n",
      "Epoch [101/200]: 100%|██████████| 125/125 [00:02<00:00, 50.31batch/s, d_loss=0.00403, g_loss=8.77] \n",
      "Epoch [102/200]: 100%|██████████| 125/125 [00:02<00:00, 62.00batch/s, d_loss=0.00107, g_loss=10.7] \n",
      "Epoch [103/200]: 100%|██████████| 125/125 [00:02<00:00, 60.25batch/s, d_loss=0.0401, g_loss=6.55] \n",
      "Epoch [104/200]: 100%|██████████| 125/125 [00:02<00:00, 60.56batch/s, d_loss=0.0157, g_loss=8.37] \n",
      "Epoch [105/200]: 100%|██████████| 125/125 [00:02<00:00, 61.24batch/s, d_loss=0.00227, g_loss=9.56] \n",
      "Epoch [106/200]: 100%|██████████| 125/125 [00:01<00:00, 63.24batch/s, d_loss=0.000842, g_loss=8.49]\n",
      "Epoch [107/200]: 100%|██████████| 125/125 [00:01<00:00, 63.20batch/s, d_loss=0.0747, g_loss=4.57] \n",
      "Epoch [108/200]: 100%|██████████| 125/125 [00:02<00:00, 61.63batch/s, d_loss=0.0944, g_loss=5.51] \n",
      "Epoch [109/200]: 100%|██████████| 125/125 [00:02<00:00, 58.82batch/s, d_loss=0.00321, g_loss=7.59]\n",
      "Epoch [110/200]: 100%|██████████| 125/125 [00:01<00:00, 63.19batch/s, d_loss=0.000176, g_loss=10.9]\n",
      "Epoch [111/200]: 100%|██████████| 125/125 [00:01<00:00, 62.88batch/s, d_loss=0.000484, g_loss=9.3] \n",
      "Epoch [112/200]: 100%|██████████| 125/125 [00:02<00:00, 57.33batch/s, d_loss=0.00472, g_loss=9.34] \n",
      "Epoch [113/200]: 100%|██████████| 125/125 [00:02<00:00, 56.39batch/s, d_loss=0.0152, g_loss=9.11] \n",
      "Epoch [114/200]: 100%|██████████| 125/125 [00:02<00:00, 59.75batch/s, d_loss=0.0276, g_loss=6.77] \n",
      "Epoch [115/200]: 100%|██████████| 125/125 [00:02<00:00, 59.17batch/s, d_loss=0.00651, g_loss=7.97] \n",
      "Epoch [116/200]: 100%|██████████| 125/125 [00:02<00:00, 53.65batch/s, d_loss=0.000359, g_loss=9.37]\n",
      "Epoch [117/200]: 100%|██████████| 125/125 [00:02<00:00, 55.70batch/s, d_loss=0.000757, g_loss=9.16]\n",
      "Epoch [118/200]: 100%|██████████| 125/125 [00:02<00:00, 59.52batch/s, d_loss=0.000102, g_loss=11.6]\n",
      "Epoch [119/200]: 100%|██████████| 125/125 [00:02<00:00, 61.72batch/s, d_loss=0.333, g_loss=3.37]  \n",
      "Epoch [120/200]: 100%|██████████| 125/125 [00:02<00:00, 61.75batch/s, d_loss=0.00849, g_loss=7.81]\n",
      "Epoch [121/200]: 100%|██████████| 125/125 [00:02<00:00, 57.15batch/s, d_loss=0.0209, g_loss=5.58] \n",
      "Epoch [122/200]: 100%|██████████| 125/125 [00:02<00:00, 62.34batch/s, d_loss=0.00902, g_loss=6.52] \n",
      "Epoch [123/200]: 100%|██████████| 125/125 [00:01<00:00, 62.55batch/s, d_loss=0.0174, g_loss=6.35]  \n",
      "Epoch [124/200]: 100%|██████████| 125/125 [00:02<00:00, 60.77batch/s, d_loss=0.215, g_loss=3.72]  \n",
      "Epoch [125/200]: 100%|██████████| 125/125 [00:02<00:00, 59.16batch/s, d_loss=0.0511, g_loss=4.68] \n",
      "Epoch [126/200]: 100%|██████████| 125/125 [00:01<00:00, 62.87batch/s, d_loss=0.288, g_loss=6.76]  \n",
      "Epoch [127/200]: 100%|██████████| 125/125 [00:01<00:00, 63.04batch/s, d_loss=0.0987, g_loss=5.83] \n",
      "Epoch [128/200]: 100%|██████████| 125/125 [00:01<00:00, 62.93batch/s, d_loss=0.0448, g_loss=6.97] \n",
      "Epoch [129/200]: 100%|██████████| 125/125 [00:02<00:00, 61.08batch/s, d_loss=0.00201, g_loss=9.06] \n",
      "Epoch [130/200]: 100%|██████████| 125/125 [00:01<00:00, 62.80batch/s, d_loss=0.00241, g_loss=6.74] \n",
      "Epoch [131/200]: 100%|██████████| 125/125 [00:01<00:00, 62.91batch/s, d_loss=0.000725, g_loss=9.08]\n",
      "Epoch [132/200]: 100%|██████████| 125/125 [00:01<00:00, 63.08batch/s, d_loss=0.00767, g_loss=10.8] \n",
      "Epoch [133/200]: 100%|██████████| 125/125 [00:02<00:00, 61.18batch/s, d_loss=0.0823, g_loss=8.53]  \n",
      "Epoch [134/200]: 100%|██████████| 125/125 [00:01<00:00, 63.03batch/s, d_loss=0.067, g_loss=6.72]  \n",
      "Epoch [135/200]: 100%|██████████| 125/125 [00:01<00:00, 62.95batch/s, d_loss=0.0331, g_loss=6.88] \n",
      "Epoch [136/200]: 100%|██████████| 125/125 [00:02<00:00, 62.43batch/s, d_loss=0.00776, g_loss=9.09]\n",
      "Epoch [137/200]: 100%|██████████| 125/125 [00:02<00:00, 61.15batch/s, d_loss=0.00591, g_loss=6.13] \n",
      "Epoch [138/200]: 100%|██████████| 125/125 [00:02<00:00, 60.83batch/s, d_loss=0.00221, g_loss=9.12] \n",
      "Epoch [139/200]: 100%|██████████| 125/125 [00:02<00:00, 62.48batch/s, d_loss=0.000117, g_loss=11.6]\n",
      "Epoch [140/200]: 100%|██████████| 125/125 [00:02<00:00, 61.20batch/s, d_loss=0.000464, g_loss=8.72]\n",
      "Epoch [141/200]: 100%|██████████| 125/125 [00:02<00:00, 61.07batch/s, d_loss=0.00729, g_loss=11.5] \n",
      "Epoch [142/200]: 100%|██████████| 125/125 [00:02<00:00, 62.11batch/s, d_loss=0.00294, g_loss=8.33] \n",
      "Epoch [143/200]: 100%|██████████| 125/125 [00:01<00:00, 63.29batch/s, d_loss=0.152, g_loss=5.01]  \n",
      "Epoch [144/200]: 100%|██████████| 125/125 [00:02<00:00, 60.51batch/s, d_loss=0.000203, g_loss=9.57]\n",
      "Epoch [145/200]: 100%|██████████| 125/125 [00:02<00:00, 60.53batch/s, d_loss=0.00171, g_loss=7.78] \n",
      "Epoch [146/200]: 100%|██████████| 125/125 [00:02<00:00, 61.69batch/s, d_loss=0.0133, g_loss=6.47]  \n",
      "Epoch [147/200]: 100%|██████████| 125/125 [00:02<00:00, 58.06batch/s, d_loss=0.000374, g_loss=9.37]\n",
      "Epoch [148/200]: 100%|██████████| 125/125 [00:02<00:00, 61.37batch/s, d_loss=0.215, g_loss=7.56]  \n",
      "Epoch [149/200]: 100%|██████████| 125/125 [00:02<00:00, 58.93batch/s, d_loss=0.0157, g_loss=7.23] \n",
      "Epoch [150/200]: 100%|██████████| 125/125 [00:02<00:00, 61.38batch/s, d_loss=0.00252, g_loss=8.1]  \n",
      "Epoch [151/200]: 100%|██████████| 125/125 [00:02<00:00, 62.39batch/s, d_loss=0.00128, g_loss=7.85] \n",
      "Epoch [152/200]: 100%|██████████| 125/125 [00:02<00:00, 62.05batch/s, d_loss=0.000536, g_loss=8.43]\n",
      "Epoch [153/200]: 100%|██████████| 125/125 [00:02<00:00, 61.11batch/s, d_loss=2.07, g_loss=10.7]    \n",
      "Epoch [154/200]: 100%|██████████| 125/125 [00:02<00:00, 58.48batch/s, d_loss=0.059, g_loss=5.61]  \n",
      "Epoch [155/200]: 100%|██████████| 125/125 [00:02<00:00, 58.57batch/s, d_loss=0.00268, g_loss=8.18] \n",
      "Epoch [156/200]: 100%|██████████| 125/125 [00:02<00:00, 58.85batch/s, d_loss=0.00421, g_loss=7.86]\n",
      "Epoch [157/200]: 100%|██████████| 125/125 [00:02<00:00, 57.84batch/s, d_loss=0.000297, g_loss=9.6] \n",
      "Epoch [158/200]: 100%|██████████| 125/125 [00:02<00:00, 61.13batch/s, d_loss=0.000747, g_loss=9.09]\n",
      "Epoch [159/200]: 100%|██████████| 125/125 [00:02<00:00, 61.83batch/s, d_loss=0.00134, g_loss=7.63] \n",
      "Epoch [160/200]: 100%|██████████| 125/125 [00:02<00:00, 61.84batch/s, d_loss=0.000418, g_loss=9.67]\n",
      "Epoch [161/200]: 100%|██████████| 125/125 [00:02<00:00, 59.53batch/s, d_loss=0.00179, g_loss=9.13] \n",
      "Epoch [162/200]: 100%|██████████| 125/125 [00:02<00:00, 61.27batch/s, d_loss=0.15, g_loss=9.78]    \n",
      "Epoch [163/200]: 100%|██████████| 125/125 [00:02<00:00, 61.91batch/s, d_loss=0.00692, g_loss=6.74]\n",
      "Epoch [164/200]: 100%|██████████| 125/125 [00:02<00:00, 61.86batch/s, d_loss=0.0268, g_loss=5.59] \n",
      "Epoch [165/200]: 100%|██████████| 125/125 [00:02<00:00, 59.40batch/s, d_loss=0.00326, g_loss=7.43]\n",
      "Epoch [166/200]: 100%|██████████| 125/125 [00:02<00:00, 62.02batch/s, d_loss=0.00189, g_loss=9.37] \n",
      "Epoch [167/200]: 100%|██████████| 125/125 [00:02<00:00, 61.78batch/s, d_loss=0.00946, g_loss=7.01] \n",
      "Epoch [168/200]: 100%|██████████| 125/125 [00:02<00:00, 62.06batch/s, d_loss=0.000936, g_loss=9.1] \n",
      "Epoch [169/200]: 100%|██████████| 125/125 [00:02<00:00, 60.29batch/s, d_loss=0.000178, g_loss=12.8]\n",
      "Epoch [170/200]: 100%|██████████| 125/125 [00:02<00:00, 61.80batch/s, d_loss=0.000293, g_loss=10.9]\n",
      "Epoch [171/200]: 100%|██████████| 125/125 [00:02<00:00, 60.34batch/s, d_loss=0.00116, g_loss=9.12] \n",
      "Epoch [172/200]: 100%|██████████| 125/125 [00:02<00:00, 61.98batch/s, d_loss=0.00103, g_loss=8.18] \n",
      "Epoch [173/200]: 100%|██████████| 125/125 [00:02<00:00, 59.85batch/s, d_loss=0.000217, g_loss=10.5]\n",
      "Epoch [174/200]: 100%|██████████| 125/125 [00:02<00:00, 62.18batch/s, d_loss=0.000777, g_loss=8.44]\n",
      "Epoch [175/200]: 100%|██████████| 125/125 [00:02<00:00, 62.14batch/s, d_loss=0.000595, g_loss=8.33]\n",
      "Epoch [176/200]: 100%|██████████| 125/125 [00:02<00:00, 61.84batch/s, d_loss=0.00049, g_loss=8.62] \n",
      "Epoch [177/200]: 100%|██████████| 125/125 [00:02<00:00, 59.59batch/s, d_loss=0.000141, g_loss=9.51]\n",
      "Epoch [178/200]: 100%|██████████| 125/125 [00:02<00:00, 61.58batch/s, d_loss=0.000628, g_loss=8.62]\n",
      "Epoch [179/200]: 100%|██████████| 125/125 [00:01<00:00, 62.54batch/s, d_loss=0.00104, g_loss=8.52] \n",
      "Epoch [180/200]: 100%|██████████| 125/125 [00:02<00:00, 62.47batch/s, d_loss=8.82e-7, g_loss=14.4] \n",
      "Epoch [181/200]: 100%|██████████| 125/125 [00:02<00:00, 58.55batch/s, d_loss=64, g_loss=100]       \n",
      "Epoch [182/200]: 100%|██████████| 125/125 [00:01<00:00, 62.95batch/s, d_loss=63.9, g_loss=100]\n",
      "Epoch [183/200]: 100%|██████████| 125/125 [00:01<00:00, 63.25batch/s, d_loss=61.3, g_loss=100]\n",
      "Epoch [184/200]: 100%|██████████| 125/125 [00:01<00:00, 63.08batch/s, d_loss=64, g_loss=100]  \n",
      "Epoch [185/200]: 100%|██████████| 125/125 [00:02<00:00, 59.85batch/s, d_loss=63.2, g_loss=100]\n",
      "Epoch [186/200]: 100%|██████████| 125/125 [00:01<00:00, 62.74batch/s, d_loss=63, g_loss=100]  \n",
      "Epoch [187/200]: 100%|██████████| 125/125 [00:01<00:00, 62.81batch/s, d_loss=63.5, g_loss=100]\n",
      "Epoch [188/200]: 100%|██████████| 125/125 [00:01<00:00, 63.22batch/s, d_loss=62.6, g_loss=100]\n",
      "Epoch [189/200]: 100%|██████████| 125/125 [00:02<00:00, 59.33batch/s, d_loss=64.2, g_loss=100]\n",
      "Epoch [190/200]: 100%|██████████| 125/125 [00:02<00:00, 62.29batch/s, d_loss=64.3, g_loss=100]\n",
      "Epoch [191/200]: 100%|██████████| 125/125 [00:02<00:00, 61.99batch/s, d_loss=62.8, g_loss=100]\n",
      "Epoch [192/200]: 100%|██████████| 125/125 [00:01<00:00, 63.00batch/s, d_loss=62.2, g_loss=100]\n",
      "Epoch [193/200]: 100%|██████████| 125/125 [00:02<00:00, 60.64batch/s, d_loss=62.5, g_loss=100]\n",
      "Epoch [194/200]: 100%|██████████| 125/125 [00:01<00:00, 62.94batch/s, d_loss=65.5, g_loss=100]\n",
      "Epoch [195/200]: 100%|██████████| 125/125 [00:01<00:00, 63.14batch/s, d_loss=62.9, g_loss=100]\n",
      "Epoch [196/200]: 100%|██████████| 125/125 [00:02<00:00, 61.91batch/s, d_loss=63.8, g_loss=100]\n",
      "Epoch [197/200]: 100%|██████████| 125/125 [00:02<00:00, 60.34batch/s, d_loss=64.9, g_loss=100]\n",
      "Epoch [198/200]: 100%|██████████| 125/125 [00:01<00:00, 62.84batch/s, d_loss=62, g_loss=100]  \n",
      "Epoch [199/200]: 100%|██████████| 125/125 [00:02<00:00, 62.46batch/s, d_loss=61.9, g_loss=100]\n",
      "Epoch [200/200]: 100%|██████████| 125/125 [00:01<00:00, 62.54batch/s, d_loss=62.2, g_loss=100]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_conv_gan = DCGAN()\n",
    "deep_conv_gan.load_dataset()\n",
    "deep_conv_gan.load_model()\n",
    "deep_conv_gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T08:59:06.633535800Z",
     "start_time": "2023-05-20T08:59:06.374655500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('Testing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b95d3b5331a269c10f88601cd1d3baff493257c4bb176e7422896ec2d1e1d22d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
