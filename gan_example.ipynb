{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.415582100Z",
     "start_time": "2023-05-20T10:29:05.982063300Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.421073400Z",
     "start_time": "2023-05-20T10:29:08.417577300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Settings(BaseModel):\n",
    "    project_name: str = 'jp50_03'\n",
    "    device: str = 'cuda'\n",
    "    epoch: int = 200\n",
    "    batch: int = 50\n",
    "    learning_rate: float = 1e-4\n",
    "    image_size: int = 64\n",
    "    sample_interval: int = 500\n",
    "\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    nz = 100\n",
    "    # Size of feature maps in generator\n",
    "    ngf = 16\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    nc = 1\n",
    "    # Size of feature maps in discriminator\n",
    "    ndf = 64\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1 = 0.5\n",
    "    # Establish convention for real and fake labels during training\n",
    "    real_label = 1.\n",
    "    fake_label = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.426300200Z",
     "start_time": "2023-05-20T10:29:08.424065100Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf,kernel_size= 4,stride= 2,padding= 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.431481300Z",
     "start_time": "2023-05-20T10:29:08.428295Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset (Data Loader)\n",
    "\n",
    "- Here use pytorch built-in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.436820900Z",
     "start_time": "2023-05-20T10:29:08.434317400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import mnist\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 調整大小為 64x64\n",
    "    transforms.Grayscale(num_output_channels=1),  # 轉為單通道灰度圖像\n",
    "    transforms.ToTensor(),  # 轉為張量\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 歸一化\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:29:08.468658400Z",
     "start_time": "2023-05-20T10:29:08.436820900Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self) -> None:\n",
    "        self.args = Settings()\n",
    "\n",
    "        wandb.init(project=self.args.project_name, config=self.args.dict(), save_code=True)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.dataset = datasets.ImageFolder(root='./dataset/train_images', transform=transform) # datasets.ImageFolder 會將資料夾中的圖片依照資料夾名稱分類\n",
    "        #訓練集\n",
    "        self.loader = DataLoader(self.dataset, batch_size=self.args.batch, shuffle=True)  #每次訓練數量 = Data set size(*0.8) / Batch size = 800/20 = 40\n",
    "\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        self.g_model, self.g_loss, self.g_optim = self.generator()\n",
    "        self.d_model, self.d_loss, self.d_optim = self.discriminator()\n",
    "\n",
    "    def generator(self):\n",
    "        model = Generator(nc=self.args.nc, ngf=self.args.ngf, nz=self.args.nz).to(self.args.device)\n",
    "        model.apply(self.weights_init)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args.learning_rate, betas=(self.args.beta1, 0.999))\n",
    "        return model, criterion, optimizer\n",
    "\n",
    "    def discriminator(self):\n",
    "        model = Discriminator(nc=self.args.nc, ndf=self.args.ndf).to(self.args.device)\n",
    "        model.apply(self.weights_init)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args.learning_rate, betas=(self.args.beta1, 0.999))\n",
    "        return model, criterion, optimizer\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        # custom weights initialization called on netG and netD\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def train_one_epoch(self, i_epoch: int):\n",
    "        self.metric = {\n",
    "            'train_d': {},\n",
    "            'train_g': {},\n",
    "        }\n",
    "        self.i_epoch = i_epoch\n",
    "        self.d_model.train(mode=True)\n",
    "        self.g_model.train(mode=True)\n",
    "        bar = tqdm(self.loader, unit='batch', leave=True)\n",
    "        for i_batch, (data, label) in enumerate(bar):\n",
    "            self.step = (i_epoch * len(self.loader) + i_batch)\n",
    "            data = data.to(self.args.device)\n",
    "\n",
    "            fake = self.train_d(data)\n",
    "\n",
    "            self.train_g(fake)\n",
    "\n",
    "            self.show_result(fake)\n",
    "\n",
    "            loss = {\n",
    "                'd_loss': self.metric['train_d']['loss'][-1],\n",
    "                'g_loss': self.metric['train_g']['loss'][-1],\n",
    "            }\n",
    "            wandb.log({**loss, **{'i_epoch': self.i_epoch, 'step': self.step, }}, step=self.step)\n",
    "\n",
    "            bar.set_description(f'Epoch [{self.i_epoch + 1}/{self.args.epoch}]')\n",
    "            bar.set_postfix(**loss)\n",
    "        return 0\n",
    "\n",
    "    def show_result(self, fake):\n",
    "        if self.step % self.args.sample_interval == 0:\n",
    "            wandb.log({\n",
    "                'fake': [wandb.Image(im.permute(1,2,0).detach().cpu().numpy()) for index, im in enumerate(fake) if index < 24]\n",
    "            }, step=self.step)\n",
    "        return 0\n",
    "\n",
    "    def train_d(self, data):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # Train with all-real batch\n",
    "        self.d_optim.zero_grad()\n",
    "        # Format batch\n",
    "        real_inputs = data.to(self.args.device)\n",
    "        b_size = real_inputs.size(0)\n",
    "        label = torch.full((b_size,), self.args.real_label, dtype=torch.float, device=self.args.device)\n",
    "        # Forward pass real batch through D\n",
    "        output = self.d_model.forward(real_inputs).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = self.d_loss.forward(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        torch.autograd.backward(errD_real)\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, self.args.nz, 1, 1, device=self.args.device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = self.g_model.forward(noise)\n",
    "        label.fill_(self.args.fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = self.d_model.forward(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = self.d_loss.forward(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        torch.autograd.backward(errD_fake)\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        self.d_optim.step()\n",
    "        self.metric['train_d'].setdefault('loss', []).append(errD.item())\n",
    "        return fake\n",
    "\n",
    "    def train_g(self, fake):\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        self.g_optim.zero_grad()\n",
    "        # fake labels are real for generator cost\n",
    "        label = torch.full((fake.size(0),), self.args.real_label, dtype=torch.float, device=self.args.device)\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = self.d_model.forward(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = self.d_loss(output, label)\n",
    "        # Calculate gradients for G\n",
    "        torch.autograd.backward(errG)\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        self.g_optim.step()\n",
    "        self.metric['train_g'].setdefault('loss', []).append(errG.item())\n",
    "        return 0\n",
    "\n",
    "    def train(self):\n",
    "        for i_epoch in range(self.args.epoch):\n",
    "            self.train_one_epoch(i_epoch)\n",
    "            self.validation()\n",
    "            self.save_model()\n",
    "        return 0\n",
    "\n",
    "    def validation(self):\n",
    "        return 0\n",
    "\n",
    "    def test(self):\n",
    "        return 0\n",
    "\n",
    "    def save_model(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T10:32:26.901816700Z",
     "start_time": "2023-05-20T10:29:08.452702300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhj6hki123\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>F:\\CODING\\Pytorch_Learning\\wandb\\run-20230520_182910-jo9cgzps</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hj6hki123/jp50_03/runs/jo9cgzps' target=\"_blank\">balmy-frog-1</a></strong> to <a href='https://wandb.ai/hj6hki123/jp50_03' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hj6hki123/jp50_03' target=\"_blank\">https://wandb.ai/hj6hki123/jp50_03</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hj6hki123/jp50_03/runs/jo9cgzps' target=\"_blank\">https://wandb.ai/hj6hki123/jp50_03/runs/jo9cgzps</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]: 100%|██████████| 20/20 [00:01<00:00, 10.25batch/s, d_loss=0.0368, g_loss=5.26]\n",
      "Epoch [2/200]: 100%|██████████| 20/20 [00:00<00:00, 20.66batch/s, d_loss=0.0144, g_loss=5.81]\n",
      "Epoch [3/200]: 100%|██████████| 20/20 [00:00<00:00, 21.75batch/s, d_loss=0.0106, g_loss=6.72]\n",
      "Epoch [4/200]: 100%|██████████| 20/20 [00:00<00:00, 21.42batch/s, d_loss=0.0127, g_loss=7.05]\n",
      "Epoch [5/200]: 100%|██████████| 20/20 [00:00<00:00, 21.52batch/s, d_loss=0.00759, g_loss=7.38]\n",
      "Epoch [6/200]: 100%|██████████| 20/20 [00:00<00:00, 21.72batch/s, d_loss=0.00589, g_loss=7.75]\n",
      "Epoch [7/200]: 100%|██████████| 20/20 [00:00<00:00, 21.73batch/s, d_loss=0.00141, g_loss=38.3] \n",
      "Epoch [8/200]: 100%|██████████| 20/20 [00:00<00:00, 21.74batch/s, d_loss=3.89e-5, g_loss=38.4] \n",
      "Epoch [9/200]: 100%|██████████| 20/20 [00:00<00:00, 21.65batch/s, d_loss=1.11e-5, g_loss=38.2] \n",
      "Epoch [10/200]: 100%|██████████| 20/20 [00:00<00:00, 21.76batch/s, d_loss=9.62e-6, g_loss=38.1] \n",
      "Epoch [11/200]: 100%|██████████| 20/20 [00:00<00:00, 21.87batch/s, d_loss=8.45e-6, g_loss=38.2]\n",
      "Epoch [12/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=6.9e-6, g_loss=38.3]  \n",
      "Epoch [13/200]: 100%|██████████| 20/20 [00:00<00:00, 21.70batch/s, d_loss=1.66e-5, g_loss=37.9] \n",
      "Epoch [14/200]: 100%|██████████| 20/20 [00:00<00:00, 21.77batch/s, d_loss=9.49e-6, g_loss=38.1]\n",
      "Epoch [15/200]: 100%|██████████| 20/20 [00:00<00:00, 21.88batch/s, d_loss=1.13e-5, g_loss=38.1] \n",
      "Epoch [16/200]: 100%|██████████| 20/20 [00:00<00:00, 21.73batch/s, d_loss=1.84e-5, g_loss=38]   \n",
      "Epoch [17/200]: 100%|██████████| 20/20 [00:00<00:00, 21.68batch/s, d_loss=1.89e-5, g_loss=37.8] \n",
      "Epoch [18/200]: 100%|██████████| 20/20 [00:00<00:00, 21.58batch/s, d_loss=2.67e-5, g_loss=37.6] \n",
      "Epoch [19/200]: 100%|██████████| 20/20 [00:00<00:00, 21.45batch/s, d_loss=1.19e-5, g_loss=37.5] \n",
      "Epoch [20/200]: 100%|██████████| 20/20 [00:00<00:00, 21.34batch/s, d_loss=2.31e-6, g_loss=37.6]\n",
      "Epoch [21/200]: 100%|██████████| 20/20 [00:00<00:00, 20.79batch/s, d_loss=3.58e-6, g_loss=37.5]\n",
      "Epoch [22/200]: 100%|██████████| 20/20 [00:00<00:00, 20.66batch/s, d_loss=2.83e-5, g_loss=37.3] \n",
      "Epoch [23/200]: 100%|██████████| 20/20 [00:00<00:00, 21.57batch/s, d_loss=1.51e-6, g_loss=37.1]\n",
      "Epoch [24/200]: 100%|██████████| 20/20 [00:00<00:00, 20.32batch/s, d_loss=7.49e-6, g_loss=37.2]\n",
      "Epoch [25/200]: 100%|██████████| 20/20 [00:00<00:00, 20.14batch/s, d_loss=5.62e-6, g_loss=37.2]\n",
      "Epoch [26/200]: 100%|██████████| 20/20 [00:01<00:00, 15.90batch/s, d_loss=4.81e-6, g_loss=37]  \n",
      "Epoch [27/200]: 100%|██████████| 20/20 [00:00<00:00, 20.49batch/s, d_loss=3.98e-7, g_loss=36.5]\n",
      "Epoch [28/200]: 100%|██████████| 20/20 [00:01<00:00, 19.53batch/s, d_loss=5.88e-6, g_loss=35.9]\n",
      "Epoch [29/200]: 100%|██████████| 20/20 [00:01<00:00, 19.84batch/s, d_loss=1.66e-6, g_loss=33.8]\n",
      "Epoch [30/200]: 100%|██████████| 20/20 [00:00<00:00, 20.29batch/s, d_loss=0.0632, g_loss=33.8] \n",
      "Epoch [31/200]: 100%|██████████| 20/20 [00:01<00:00, 19.87batch/s, d_loss=0.00141, g_loss=30.4]\n",
      "Epoch [32/200]: 100%|██████████| 20/20 [00:01<00:00, 19.61batch/s, d_loss=0.000199, g_loss=31.3]\n",
      "Epoch [33/200]: 100%|██████████| 20/20 [00:01<00:00, 19.39batch/s, d_loss=1.2, g_loss=29.9]    \n",
      "Epoch [34/200]: 100%|██████████| 20/20 [00:01<00:00, 19.72batch/s, d_loss=0.159, g_loss=9.28] \n",
      "Epoch [35/200]: 100%|██████████| 20/20 [00:00<00:00, 20.35batch/s, d_loss=0.0388, g_loss=6.02]\n",
      "Epoch [36/200]: 100%|██████████| 20/20 [00:00<00:00, 20.82batch/s, d_loss=0.021, g_loss=6.29] \n",
      "Epoch [37/200]: 100%|██████████| 20/20 [00:00<00:00, 21.33batch/s, d_loss=0.0207, g_loss=6.52]\n",
      "Epoch [38/200]: 100%|██████████| 20/20 [00:00<00:00, 21.49batch/s, d_loss=0.014, g_loss=6.76]  \n",
      "Epoch [39/200]: 100%|██████████| 20/20 [00:00<00:00, 21.40batch/s, d_loss=0.0108, g_loss=7.17] \n",
      "Epoch [40/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=0.00407, g_loss=7.39]\n",
      "Epoch [41/200]: 100%|██████████| 20/20 [00:00<00:00, 21.85batch/s, d_loss=0.00891, g_loss=7.33]\n",
      "Epoch [42/200]: 100%|██████████| 20/20 [00:00<00:00, 20.37batch/s, d_loss=0.00651, g_loss=7.11]\n",
      "Epoch [43/200]: 100%|██████████| 20/20 [00:00<00:00, 20.47batch/s, d_loss=0.00503, g_loss=7.05]\n",
      "Epoch [44/200]: 100%|██████████| 20/20 [00:00<00:00, 20.61batch/s, d_loss=0.00449, g_loss=7.08]\n",
      "Epoch [45/200]: 100%|██████████| 20/20 [00:00<00:00, 21.19batch/s, d_loss=0.00427, g_loss=7.04]\n",
      "Epoch [46/200]: 100%|██████████| 20/20 [00:00<00:00, 21.71batch/s, d_loss=0.00385, g_loss=6.98]\n",
      "Epoch [47/200]: 100%|██████████| 20/20 [00:00<00:00, 21.87batch/s, d_loss=0.00434, g_loss=7.15]\n",
      "Epoch [48/200]: 100%|██████████| 20/20 [00:00<00:00, 21.28batch/s, d_loss=0.00383, g_loss=7.14]\n",
      "Epoch [49/200]: 100%|██████████| 20/20 [00:00<00:00, 21.35batch/s, d_loss=0.00403, g_loss=7.23]\n",
      "Epoch [50/200]: 100%|██████████| 20/20 [00:00<00:00, 21.13batch/s, d_loss=0.00424, g_loss=7]   \n",
      "Epoch [51/200]: 100%|██████████| 20/20 [00:01<00:00, 17.55batch/s, d_loss=0.00523, g_loss=7.27]\n",
      "Epoch [52/200]: 100%|██████████| 20/20 [00:00<00:00, 21.87batch/s, d_loss=0.00788, g_loss=6.84]\n",
      "Epoch [53/200]: 100%|██████████| 20/20 [00:00<00:00, 20.62batch/s, d_loss=0.00699, g_loss=7.04]\n",
      "Epoch [54/200]: 100%|██████████| 20/20 [00:00<00:00, 20.27batch/s, d_loss=0.00946, g_loss=6.32]\n",
      "Epoch [55/200]: 100%|██████████| 20/20 [00:00<00:00, 20.48batch/s, d_loss=0.00452, g_loss=7.08]\n",
      "Epoch [56/200]: 100%|██████████| 20/20 [00:00<00:00, 21.48batch/s, d_loss=0.00287, g_loss=7.33]\n",
      "Epoch [57/200]: 100%|██████████| 20/20 [00:00<00:00, 21.17batch/s, d_loss=0.00373, g_loss=7.1] \n",
      "Epoch [58/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=0.00303, g_loss=7.14]\n",
      "Epoch [59/200]: 100%|██████████| 20/20 [00:00<00:00, 21.12batch/s, d_loss=0.00334, g_loss=7.35]\n",
      "Epoch [60/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=0.00337, g_loss=7.18]\n",
      "Epoch [61/200]: 100%|██████████| 20/20 [00:00<00:00, 21.12batch/s, d_loss=0.00437, g_loss=7.24]\n",
      "Epoch [62/200]: 100%|██████████| 20/20 [00:00<00:00, 22.05batch/s, d_loss=0.00703, g_loss=9.64]\n",
      "Epoch [63/200]: 100%|██████████| 20/20 [00:00<00:00, 21.90batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [64/200]: 100%|██████████| 20/20 [00:00<00:00, 21.08batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [65/200]: 100%|██████████| 20/20 [00:00<00:00, 21.19batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [66/200]: 100%|██████████| 20/20 [00:00<00:00, 21.07batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [67/200]: 100%|██████████| 20/20 [00:00<00:00, 21.17batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [68/200]: 100%|██████████| 20/20 [00:00<00:00, 21.47batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [69/200]: 100%|██████████| 20/20 [00:00<00:00, 21.05batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [70/200]: 100%|██████████| 20/20 [00:00<00:00, 21.90batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [71/200]: 100%|██████████| 20/20 [00:00<00:00, 21.82batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [72/200]: 100%|██████████| 20/20 [00:00<00:00, 21.85batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [73/200]: 100%|██████████| 20/20 [00:00<00:00, 21.97batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [74/200]: 100%|██████████| 20/20 [00:00<00:00, 21.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [75/200]: 100%|██████████| 20/20 [00:00<00:00, 21.40batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [76/200]: 100%|██████████| 20/20 [00:01<00:00, 17.34batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [77/200]: 100%|██████████| 20/20 [00:00<00:00, 21.18batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [78/200]: 100%|██████████| 20/20 [00:00<00:00, 21.46batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [79/200]: 100%|██████████| 20/20 [00:00<00:00, 21.76batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [80/200]: 100%|██████████| 20/20 [00:00<00:00, 21.87batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [81/200]: 100%|██████████| 20/20 [00:00<00:00, 21.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [82/200]: 100%|██████████| 20/20 [00:00<00:00, 21.86batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [83/200]: 100%|██████████| 20/20 [00:00<00:00, 21.86batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [84/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [85/200]: 100%|██████████| 20/20 [00:00<00:00, 21.97batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [86/200]: 100%|██████████| 20/20 [00:00<00:00, 21.38batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [87/200]: 100%|██████████| 20/20 [00:00<00:00, 21.06batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [88/200]: 100%|██████████| 20/20 [00:00<00:00, 20.97batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [89/200]: 100%|██████████| 20/20 [00:00<00:00, 21.52batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [90/200]: 100%|██████████| 20/20 [00:00<00:00, 21.88batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [91/200]: 100%|██████████| 20/20 [00:00<00:00, 21.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [92/200]: 100%|██████████| 20/20 [00:00<00:00, 22.02batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [93/200]: 100%|██████████| 20/20 [00:00<00:00, 21.97batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [94/200]: 100%|██████████| 20/20 [00:00<00:00, 21.82batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [95/200]: 100%|██████████| 20/20 [00:00<00:00, 22.12batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [96/200]: 100%|██████████| 20/20 [00:00<00:00, 22.01batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [97/200]: 100%|██████████| 20/20 [00:00<00:00, 21.62batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [98/200]: 100%|██████████| 20/20 [00:00<00:00, 20.60batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [99/200]: 100%|██████████| 20/20 [00:00<00:00, 20.48batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [100/200]: 100%|██████████| 20/20 [00:00<00:00, 21.09batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [101/200]: 100%|██████████| 20/20 [00:01<00:00, 18.01batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [102/200]: 100%|██████████| 20/20 [00:00<00:00, 21.87batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [103/200]: 100%|██████████| 20/20 [00:00<00:00, 21.76batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [104/200]: 100%|██████████| 20/20 [00:00<00:00, 21.85batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [105/200]: 100%|██████████| 20/20 [00:00<00:00, 21.85batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [106/200]: 100%|██████████| 20/20 [00:00<00:00, 21.57batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [107/200]: 100%|██████████| 20/20 [00:00<00:00, 21.74batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [108/200]: 100%|██████████| 20/20 [00:00<00:00, 21.08batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [109/200]: 100%|██████████| 20/20 [00:00<00:00, 20.12batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [110/200]: 100%|██████████| 20/20 [00:00<00:00, 20.71batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [111/200]: 100%|██████████| 20/20 [00:00<00:00, 21.01batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [112/200]: 100%|██████████| 20/20 [00:00<00:00, 21.12batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [113/200]: 100%|██████████| 20/20 [00:00<00:00, 21.31batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [114/200]: 100%|██████████| 20/20 [00:00<00:00, 20.99batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [115/200]: 100%|██████████| 20/20 [00:00<00:00, 21.12batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [116/200]: 100%|██████████| 20/20 [00:00<00:00, 21.28batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [117/200]: 100%|██████████| 20/20 [00:00<00:00, 21.34batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [118/200]: 100%|██████████| 20/20 [00:00<00:00, 21.74batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [119/200]: 100%|██████████| 20/20 [00:00<00:00, 21.45batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [120/200]: 100%|██████████| 20/20 [00:00<00:00, 21.12batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [121/200]: 100%|██████████| 20/20 [00:00<00:00, 21.08batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [122/200]: 100%|██████████| 20/20 [00:00<00:00, 21.16batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [123/200]: 100%|██████████| 20/20 [00:00<00:00, 21.65batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [124/200]: 100%|██████████| 20/20 [00:00<00:00, 21.80batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [125/200]: 100%|██████████| 20/20 [00:00<00:00, 21.80batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [126/200]: 100%|██████████| 20/20 [00:01<00:00, 17.56batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [127/200]: 100%|██████████| 20/20 [00:00<00:00, 21.42batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [128/200]: 100%|██████████| 20/20 [00:00<00:00, 21.36batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [129/200]: 100%|██████████| 20/20 [00:00<00:00, 21.63batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [130/200]: 100%|██████████| 20/20 [00:00<00:00, 20.97batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [131/200]: 100%|██████████| 20/20 [00:00<00:00, 20.60batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [132/200]: 100%|██████████| 20/20 [00:00<00:00, 20.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [133/200]: 100%|██████████| 20/20 [00:00<00:00, 21.31batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [134/200]: 100%|██████████| 20/20 [00:00<00:00, 21.51batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [135/200]: 100%|██████████| 20/20 [00:00<00:00, 21.63batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [136/200]: 100%|██████████| 20/20 [00:00<00:00, 21.69batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [137/200]: 100%|██████████| 20/20 [00:00<00:00, 21.58batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [138/200]: 100%|██████████| 20/20 [00:00<00:00, 21.40batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [139/200]: 100%|██████████| 20/20 [00:00<00:00, 21.67batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [140/200]: 100%|██████████| 20/20 [00:00<00:00, 21.65batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [141/200]: 100%|██████████| 20/20 [00:00<00:00, 21.37batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [142/200]: 100%|██████████| 20/20 [00:00<00:00, 20.66batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [143/200]: 100%|██████████| 20/20 [00:00<00:00, 21.13batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [144/200]: 100%|██████████| 20/20 [00:00<00:00, 21.29batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [145/200]: 100%|██████████| 20/20 [00:00<00:00, 21.82batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [146/200]: 100%|██████████| 20/20 [00:00<00:00, 21.62batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [147/200]: 100%|██████████| 20/20 [00:00<00:00, 20.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [148/200]: 100%|██████████| 20/20 [00:00<00:00, 21.24batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [149/200]: 100%|██████████| 20/20 [00:00<00:00, 22.04batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [150/200]: 100%|██████████| 20/20 [00:00<00:00, 21.90batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [151/200]: 100%|██████████| 20/20 [00:01<00:00, 18.15batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [152/200]: 100%|██████████| 20/20 [00:00<00:00, 21.95batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [153/200]: 100%|██████████| 20/20 [00:00<00:00, 21.81batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [154/200]: 100%|██████████| 20/20 [00:00<00:00, 21.67batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [155/200]: 100%|██████████| 20/20 [00:00<00:00, 21.91batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [156/200]: 100%|██████████| 20/20 [00:00<00:00, 21.73batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [157/200]: 100%|██████████| 20/20 [00:00<00:00, 22.00batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [158/200]: 100%|██████████| 20/20 [00:00<00:00, 21.94batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [159/200]: 100%|██████████| 20/20 [00:00<00:00, 21.88batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [160/200]: 100%|██████████| 20/20 [00:00<00:00, 21.89batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [161/200]: 100%|██████████| 20/20 [00:00<00:00, 21.89batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [162/200]: 100%|██████████| 20/20 [00:00<00:00, 22.00batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [163/200]: 100%|██████████| 20/20 [00:00<00:00, 21.93batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [164/200]: 100%|██████████| 20/20 [00:00<00:00, 21.96batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [165/200]: 100%|██████████| 20/20 [00:00<00:00, 21.81batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [166/200]: 100%|██████████| 20/20 [00:00<00:00, 21.58batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [167/200]: 100%|██████████| 20/20 [00:00<00:00, 21.72batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [168/200]: 100%|██████████| 20/20 [00:00<00:00, 21.75batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [169/200]: 100%|██████████| 20/20 [00:00<00:00, 21.41batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [170/200]: 100%|██████████| 20/20 [00:00<00:00, 21.47batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [171/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [172/200]: 100%|██████████| 20/20 [00:00<00:00, 21.40batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [173/200]: 100%|██████████| 20/20 [00:00<00:00, 21.67batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [174/200]: 100%|██████████| 20/20 [00:00<00:00, 21.58batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [175/200]: 100%|██████████| 20/20 [00:00<00:00, 21.50batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [176/200]: 100%|██████████| 20/20 [00:01<00:00, 18.07batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [177/200]: 100%|██████████| 20/20 [00:00<00:00, 21.36batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [178/200]: 100%|██████████| 20/20 [00:00<00:00, 21.53batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [179/200]: 100%|██████████| 20/20 [00:00<00:00, 21.50batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [180/200]: 100%|██████████| 20/20 [00:00<00:00, 21.67batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [181/200]: 100%|██████████| 20/20 [00:00<00:00, 21.40batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [182/200]: 100%|██████████| 20/20 [00:00<00:00, 21.72batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [183/200]: 100%|██████████| 20/20 [00:00<00:00, 21.29batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [184/200]: 100%|██████████| 20/20 [00:00<00:00, 21.98batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [185/200]: 100%|██████████| 20/20 [00:00<00:00, 21.73batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [186/200]: 100%|██████████| 20/20 [00:00<00:00, 21.68batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [187/200]: 100%|██████████| 20/20 [00:00<00:00, 21.69batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [188/200]: 100%|██████████| 20/20 [00:00<00:00, 21.92batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [189/200]: 100%|██████████| 20/20 [00:00<00:00, 21.68batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [190/200]: 100%|██████████| 20/20 [00:00<00:00, 21.79batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [191/200]: 100%|██████████| 20/20 [00:00<00:00, 21.76batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [192/200]: 100%|██████████| 20/20 [00:00<00:00, 21.34batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [193/200]: 100%|██████████| 20/20 [00:00<00:00, 21.83batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [194/200]: 100%|██████████| 20/20 [00:00<00:00, 21.88batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [195/200]: 100%|██████████| 20/20 [00:00<00:00, 21.67batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [196/200]: 100%|██████████| 20/20 [00:00<00:00, 21.57batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [197/200]: 100%|██████████| 20/20 [00:00<00:00, 21.53batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [198/200]: 100%|██████████| 20/20 [00:00<00:00, 21.63batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [199/200]: 100%|██████████| 20/20 [00:00<00:00, 21.58batch/s, d_loss=100, g_loss=0]\n",
      "Epoch [200/200]: 100%|██████████| 20/20 [00:00<00:00, 21.66batch/s, d_loss=100, g_loss=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_conv_gan = DCGAN()\n",
    "deep_conv_gan.load_dataset()\n",
    "deep_conv_gan.load_model()\n",
    "deep_conv_gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T10:32:27.150990900Z",
     "start_time": "2023-05-20T10:32:26.902816Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('Testing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b95d3b5331a269c10f88601cd1d3baff493257c4bb176e7422896ec2d1e1d22d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
