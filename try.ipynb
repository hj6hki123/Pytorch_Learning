{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 圖片分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:50.845873300Z",
     "start_time": "2023-05-08T06:37:50.073841700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 建立目標資料夾\n",
    "if not os.path.exists('./my_images'):\n",
    "    os.mkdir('./my_images')\n",
    "\n",
    "# 遍歷資料夾中的檔案\n",
    "for filename in os.listdir('./hiragana_images'):\n",
    "    # 判斷檔案是否為 jpg 檔\n",
    "    if filename.endswith('.jpg'):\n",
    "        # 取得檔名中的非數字字元\n",
    "        class_name = os.path.splitext(''.join(filter(lambda x: not x.isdigit(), filename)))[0]\n",
    "        # 建立目標資料夾\n",
    "        target_folder = os.path.join('./my_images', class_name)\n",
    "        if not os.path.exists(target_folder):\n",
    "            os.mkdir(target_folder)\n",
    "        # 複製檔案到對應的資料夾中\n",
    "        shutil.copy(os.path.join('./hiragana_images', filename), os.path.join(target_folder, filename))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.439440100Z",
     "start_time": "2023-05-08T06:37:50.847873500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.447059300Z",
     "start_time": "2023-05-08T06:37:53.439440100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 超參數\n",
    "num_classes = 50  # 50音分類\n",
    "batch_size = 20  #訓練樣本數\n",
    "num_epochs = 10  #一期訓練  Iteration = （Data set size / Batch size）* Epoch = (1000/20)*10 = 5000\n",
    "learning_rate = 0.001 #學習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.447059300Z",
     "start_time": "2023-05-08T06:37:53.444212800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 數據預處理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # 調整大小為 32x32\n",
    "    transforms.Grayscale(num_output_channels=1),  # 轉為單通道灰度圖像\n",
    "    transforms.ToTensor()  # 轉為張量\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.951026700Z",
     "start_time": "2023-05-08T06:37:53.449062200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 載入數據 並將數據分配測試集和訓練集 8:2\n",
    "train_dataset = datasets.ImageFolder(root='./my_images', transform=transform)\n",
    "train_data, test_data = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "#訓練集\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "#測試集\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 載入數據\n",
    "# train_dataset = datasets.ImageFolder(root='F:\\pycharm_pro\\pytorch_learn\\my_images', transform=transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.958731700Z",
     "start_time": "2023-05-08T06:37:53.954027100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定義模型\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(8*8*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:53.962441400Z",
     "start_time": "2023-05-08T06:37:53.958731700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 實例化模型、損失函數和優化器\n",
    "model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:57.426269900Z",
     "start_time": "2023-05-08T06:37:53.964446400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],                     Step [10/40],                     Loss: 2.9651\n",
      "Epoch [1/10],                     Step [20/40],                     Loss: 1.6237\n",
      "Epoch [1/10],                     Step [30/40],                     Loss: 1.1094\n",
      "Epoch [1/10],                     Step [40/40],                     Loss: 0.6692\n",
      "Epoch [2/10],                     Step [10/40],                     Loss: 0.3887\n",
      "Epoch [2/10],                     Step [20/40],                     Loss: 0.3478\n",
      "Epoch [2/10],                     Step [30/40],                     Loss: 0.0869\n",
      "Epoch [2/10],                     Step [40/40],                     Loss: 0.1917\n",
      "Epoch [3/10],                     Step [10/40],                     Loss: 0.1839\n",
      "Epoch [3/10],                     Step [20/40],                     Loss: 0.0346\n",
      "Epoch [3/10],                     Step [30/40],                     Loss: 0.0680\n",
      "Epoch [3/10],                     Step [40/40],                     Loss: 0.0541\n",
      "Epoch [4/10],                     Step [10/40],                     Loss: 0.0230\n",
      "Epoch [4/10],                     Step [20/40],                     Loss: 0.0176\n",
      "Epoch [4/10],                     Step [30/40],                     Loss: 0.0482\n",
      "Epoch [4/10],                     Step [40/40],                     Loss: 0.0351\n",
      "Epoch [5/10],                     Step [10/40],                     Loss: 0.0144\n",
      "Epoch [5/10],                     Step [20/40],                     Loss: 0.0131\n",
      "Epoch [5/10],                     Step [30/40],                     Loss: 0.0125\n",
      "Epoch [5/10],                     Step [40/40],                     Loss: 0.0072\n",
      "Epoch [6/10],                     Step [10/40],                     Loss: 0.0050\n",
      "Epoch [6/10],                     Step [20/40],                     Loss: 0.0064\n",
      "Epoch [6/10],                     Step [30/40],                     Loss: 0.0074\n",
      "Epoch [6/10],                     Step [40/40],                     Loss: 0.0144\n",
      "Epoch [7/10],                     Step [10/40],                     Loss: 0.0034\n",
      "Epoch [7/10],                     Step [20/40],                     Loss: 0.0049\n",
      "Epoch [7/10],                     Step [30/40],                     Loss: 0.0052\n",
      "Epoch [7/10],                     Step [40/40],                     Loss: 0.0058\n",
      "Epoch [8/10],                     Step [10/40],                     Loss: 0.0083\n",
      "Epoch [8/10],                     Step [20/40],                     Loss: 0.0067\n",
      "Epoch [8/10],                     Step [30/40],                     Loss: 0.0038\n",
      "Epoch [8/10],                     Step [40/40],                     Loss: 0.0055\n",
      "Epoch [9/10],                     Step [10/40],                     Loss: 0.0059\n",
      "Epoch [9/10],                     Step [20/40],                     Loss: 0.0039\n",
      "Epoch [9/10],                     Step [30/40],                     Loss: 0.0029\n",
      "Epoch [9/10],                     Step [40/40],                     Loss: 0.0032\n",
      "Epoch [10/10],                     Step [10/40],                     Loss: 0.0043\n",
      "Epoch [10/10],                     Step [20/40],                     Loss: 0.0034\n",
      "Epoch [10/10],                     Step [30/40],                     Loss: 0.0033\n",
      "Epoch [10/10],                     Step [40/40],                     Loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "# 訓練模型\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 前向傳播\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向傳播和優化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
    "                    Step [{i+1}/{total_step}], \\\n",
    "                    Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:57.463081900Z",
     "start_time": "2023-05-08T06:37:57.427270200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.5 %\n"
     ]
    }
   ],
   "source": [
    "# 測試模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:57.463081900Z",
     "start_time": "2023-05-08T06:37:57.460053400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), '50on_model.ckpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 手寫測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T06:37:57.475044200Z",
     "start_time": "2023-05-08T06:37:57.466092Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAABUCAIAAABx1ZzvAAAD+0lEQVR4nO1Z25KrIBAcAU3+/2+jguehy65eiJusuZjKmX5IEUWZYW49aOZwOBwOh8PhcDgcDofD4XA4HA6H4+PRdR0Gfd/jbwiBdznGgJO/BDFGqgQN+75XVan/V2meUjKzEEKlVWt5vfIlgJ+nlKC/bgGcv7L8l0D1jDHO87ysaKd9m/JmNgzDsizTNIUQqPOyLF3Xnc9nMwshxBgPlfHZgHkvlwu8upQCzdvfbwN1m6Yp5wzDhhBKKbgeY8R2fLr+CEhGLytWdR1YliWlNI4jFIa2mMYdof/3fT/PM5+tkr9WvrZSvBUqDRRjoKaU4NhmlnPGNNylxCo6DD6OoyY/TEBdIFAjcffdykON0+nEv+QkSk5SSqUUZvJ5nnWbsAspJW4WjG9mMDu9g0uQAnZdV230+0DHhjSUgFbqug5pHJ6sjJWT9SnVAVuAjFBK2bItjf8+aITrRfKwcRw5rZRSMXNbY8Eaj8V1hEmMcRzHeZ5TSnQcev4wDK9W8wquthbchRBCSomxil/qrEHL98B6VZrEG5j8OKjWrbLAa1HZXNMshGO50kc0UdPUGgK6oTDp5XKxpuDTX46hPWphDCojU1CNYW1Lqu3ThM9pmjJzzkqBuHHH6A+xcs62mhHyqd/uk6xNDbqz8Km2wpsEy0sKHt6L4IRApRTozHL1yPJb0YRVkPBNnKuqLNbkgiejWmZZlpwzsxE9dkfv1Xaytu7C+Xyugh/TSC605u/X7XfhuLbybSR2lXWHt29VTQzoa6WUUso0TRg/HmK3oayLYqkLVAdPj0CZkhY/rt73fYwRrOnl3Ab6aMqheTUzm9SqHe8nV6m6g8qe8zwj5yPRYs6rivw4jsMw5JzZb9lP8z6rkdL3qM7YFHJnW50fzOeF6Y0bDLVp2KpQdyv++v6q292iupxMkp9zRpZ9QpxrnSSjZiVr+cyBgMHB8Kv6qoe/f7OEhivUZlDZB5wcqmwVcSZBrJjibegXAr6dwJIHdIsN2A63PG+LJt8FplmktK3EdhSqhr89z9Owv0tgphYTomZmPHIw2cjXEsZboG3V7PDK/S6pqQIGv5o5D9dcB63Z8XnH7pdTjz7wTM55mibSKQyOOSH5KZitDjgMg+a5KlXd5e1slWwl6srSH8ocz0bbpWkHZWs43Fvntfvd6k+qwVH4pZ+/evc2NMLV1GQ4bW/8fmz187ZbTmYLzMZngNZnkNiPZTJX+/lH5WR5IB/WNT7Q5ry4X07NFuDD1YveeuJ7H6ovH/vlZGzzbIAsnW6DknY4b7/az++UE/HDOCfUcw7P6oqtfv4hObv1IyH/6tg+IM7bft4+TE6Hw+FwOBwOh8PhcDgcDofjv8Q/cm5pZdkv5aIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=83x84>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測結果:kanaHE\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# 轉換圖片\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # 調整大小為 32x32\n",
    "    transforms.Grayscale(num_output_channels=1),  # 轉為單通道灰度圖像\n",
    "    transforms.ToTensor()  # 轉為張量\n",
    "])\n",
    "\n",
    "# 載入圖片\n",
    "img = Image.open('./test_images/HE.jpg')\n",
    "display(img)\n",
    "img = test_transform(img)\n",
    "\n",
    "\n",
    "# 增加一維，批次大小設為 1\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "# 使用訓練好的模型進行預測\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "    pred = output.argmax(dim=1)\n",
    "\n",
    "\n",
    "label_dict = train_dataset.class_to_idx\n",
    "reverse_label_dict = {v: k for k, v in label_dict.items()}\n",
    "print(f'預測結果:{reverse_label_dict[pred.item()]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
