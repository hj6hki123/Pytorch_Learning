{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from pydantic import BaseModel\n",
    "import wandb\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:15:55.892559800Z",
     "start_time": "2023-05-24T05:15:55.871108500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>F:\\CODING\\Pytorch_Learning\\wandb\\run-20230524_131555-ffak0xsb</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb' target=\"_blank\">breezy-frog-21</a></strong> to <a href='https://wandb.ai/hj6hki123/gan_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hj6hki123/gan_training' target=\"_blank\">https://wandb.ai/hj6hki123/gan_training</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb' target=\"_blank\">https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x1f1706bea90>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parameters(BaseModel):\n",
    "    # Hyperparameters etc.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    lr = 2e-4\n",
    "    z_dim = 64\n",
    "    image_size = 32  # 32*32\n",
    "    # image_channel = 1\n",
    "    batch_size = 10\n",
    "    num_epochs = 30\n",
    "\n",
    "P = Parameters()\n",
    "\n",
    "wandb.init(project=\"gan_training\", save_code=True,config=P.dict())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:16:01.629486200Z",
     "start_time": "2023-05-24T05:15:55.874607300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "W_{new} = {W_{o} - Kernelsize + (2 \\times Padding) \\over Stride} +1\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#鑑別器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential( #<--32\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1), #-->(32-4+2)/2 +1 = 16\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),#-->(16-4+2)/2 +1= 8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),#-->(8-4+2)/2 +1= 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),#-->(4-4)/1 +1= 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:16:01.920969900Z",
     "start_time": "2023-05-24T05:16:01.634472800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ConvTranspose2d\n",
    "output_size = (input_size - 1) * stride - 2 * padding + kernel_size + output_padding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(  #<--1\n",
    "            nn.ConvTranspose2d(z_dim, 256, kernel_size=4, stride=1, padding=0), #-->4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),#-->8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2,padding=1),#-->16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),#-->32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        return img\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:16:02.177926900Z",
     "start_time": "2023-05-24T05:16:01.923962Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((P.image_size, P.image_size)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(root='./dataset/train_images', transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=P.batch_size, shuffle=True)  #每次訓練數量 = Data size / Batch size\n",
    "\n",
    "# 初始化生成器和鑑別器\n",
    "generator = Generator(P.z_dim).to(P.device)\n",
    "discriminator = Discriminator().to(P.device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=P.lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=P.lr, betas=(0.5, 0.999))\n",
    "\n",
    "step=0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:16:02.549452600Z",
     "start_time": "2023-05-24T05:16:02.181915800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch 15/100, Loss D: 0.05296773836016655, Loss G: 4.11497688293457\n",
      "Epoch [1/30], Batch 30/100, Loss D: 0.027065562084317207, Loss G: 4.728980541229248\n",
      "Epoch [1/30], Batch 45/100, Loss D: 0.004367826506495476, Loss G: 5.584713935852051\n",
      "Epoch [1/30], Batch 60/100, Loss D: 0.00414587277919054, Loss G: 5.7506632804870605\n",
      "Epoch [1/30], Batch 75/100, Loss D: 0.0020484505221247673, Loss G: 6.435426235198975\n",
      "Epoch [1/30], Batch 90/100, Loss D: 0.0014907813165336847, Loss G: 6.783929347991943\n",
      "Epoch [2/30], Batch 15/100, Loss D: 0.001006641541607678, Loss G: 6.898268222808838\n",
      "Epoch [2/30], Batch 30/100, Loss D: 0.0012075353879481554, Loss G: 7.024201393127441\n",
      "Epoch [2/30], Batch 45/100, Loss D: 0.0005432592006400228, Loss G: 7.40516996383667\n",
      "Epoch [2/30], Batch 60/100, Loss D: 0.0005184882902540267, Loss G: 7.430285930633545\n",
      "Epoch [2/30], Batch 75/100, Loss D: 0.00037633703323081136, Loss G: 7.673314094543457\n",
      "Epoch [2/30], Batch 90/100, Loss D: 0.00037225382402539253, Loss G: 7.617879390716553\n",
      "Epoch [3/30], Batch 15/100, Loss D: 0.0006419162382371724, Loss G: 7.397387981414795\n",
      "Epoch [3/30], Batch 30/100, Loss D: 0.002216457622125745, Loss G: 7.955329418182373\n",
      "Epoch [3/30], Batch 45/100, Loss D: 0.0010210330365225673, Loss G: 7.666606426239014\n",
      "Epoch [3/30], Batch 60/100, Loss D: 0.002470957115292549, Loss G: 7.2774658203125\n",
      "Epoch [3/30], Batch 75/100, Loss D: 0.0007850409601815045, Loss G: 7.633336544036865\n",
      "Epoch [3/30], Batch 90/100, Loss D: 0.000540664535947144, Loss G: 7.624526500701904\n",
      "Epoch [4/30], Batch 15/100, Loss D: 0.00019556244660634547, Loss G: 8.347938537597656\n",
      "Epoch [4/30], Batch 30/100, Loss D: 0.0004565789131447673, Loss G: 7.700136661529541\n",
      "Epoch [4/30], Batch 45/100, Loss D: 0.0002972736256197095, Loss G: 8.147063255310059\n",
      "Epoch [4/30], Batch 60/100, Loss D: 0.000252183701377362, Loss G: 8.158408164978027\n",
      "Epoch [4/30], Batch 75/100, Loss D: 0.0003384892479516566, Loss G: 8.402414321899414\n",
      "Epoch [4/30], Batch 90/100, Loss D: 0.00015913843526504934, Loss G: 8.45878791809082\n",
      "Epoch [5/30], Batch 15/100, Loss D: 0.0005464012501761317, Loss G: 10.731611251831055\n",
      "Epoch [5/30], Batch 30/100, Loss D: 0.001171116135083139, Loss G: 11.115802764892578\n",
      "Epoch [5/30], Batch 45/100, Loss D: 0.001574816880747676, Loss G: 9.474211692810059\n",
      "Epoch [5/30], Batch 60/100, Loss D: 0.0031434844713658094, Loss G: 9.127339363098145\n",
      "Epoch [5/30], Batch 75/100, Loss D: 0.033747948706150055, Loss G: 7.8651580810546875\n",
      "Epoch [5/30], Batch 90/100, Loss D: 1.28421151638031, Loss G: 15.688374519348145\n",
      "Epoch [6/30], Batch 15/100, Loss D: 0.044669944792985916, Loss G: 5.061584949493408\n",
      "Epoch [6/30], Batch 30/100, Loss D: 0.0758962631225586, Loss G: 5.502051830291748\n",
      "Epoch [6/30], Batch 45/100, Loss D: 0.041727762669324875, Loss G: 6.1701884269714355\n",
      "Epoch [6/30], Batch 60/100, Loss D: 0.722381591796875, Loss G: 2.9959704875946045\n",
      "Epoch [6/30], Batch 75/100, Loss D: 0.13450060784816742, Loss G: 5.041130065917969\n",
      "Epoch [6/30], Batch 90/100, Loss D: 0.1364133656024933, Loss G: 3.8450658321380615\n",
      "Epoch [7/30], Batch 15/100, Loss D: 0.21879267692565918, Loss G: 2.8933169841766357\n",
      "Epoch [7/30], Batch 30/100, Loss D: 0.171542227268219, Loss G: 3.1762444972991943\n",
      "Epoch [7/30], Batch 45/100, Loss D: 0.20219573378562927, Loss G: 2.8343265056610107\n",
      "Epoch [7/30], Batch 60/100, Loss D: 0.206924170255661, Loss G: 3.4929680824279785\n",
      "Epoch [7/30], Batch 75/100, Loss D: 0.9314932227134705, Loss G: 7.75329065322876\n",
      "Epoch [7/30], Batch 90/100, Loss D: 0.36555030941963196, Loss G: 2.62569260597229\n",
      "Epoch [8/30], Batch 15/100, Loss D: 0.3376876711845398, Loss G: 5.203962802886963\n",
      "Epoch [8/30], Batch 30/100, Loss D: 0.16370119154453278, Loss G: 3.384080648422241\n",
      "Epoch [8/30], Batch 45/100, Loss D: 0.07434011250734329, Loss G: 4.2146100997924805\n",
      "Epoch [8/30], Batch 60/100, Loss D: 0.11770012974739075, Loss G: 3.7006027698516846\n",
      "Epoch [8/30], Batch 75/100, Loss D: 0.08618062734603882, Loss G: 3.334333896636963\n",
      "Epoch [8/30], Batch 90/100, Loss D: 0.13241052627563477, Loss G: 3.78559947013855\n",
      "Epoch [9/30], Batch 15/100, Loss D: 0.1270749866962433, Loss G: 5.450580596923828\n",
      "Epoch [9/30], Batch 30/100, Loss D: 0.1232149600982666, Loss G: 4.203101634979248\n",
      "Epoch [9/30], Batch 45/100, Loss D: 0.2935808002948761, Loss G: 4.355531215667725\n",
      "Epoch [9/30], Batch 60/100, Loss D: 0.14410144090652466, Loss G: 3.5473763942718506\n",
      "Epoch [9/30], Batch 75/100, Loss D: 0.09841159731149673, Loss G: 3.120816469192505\n",
      "Epoch [9/30], Batch 90/100, Loss D: 0.10725949704647064, Loss G: 3.7553298473358154\n",
      "Epoch [10/30], Batch 15/100, Loss D: 0.09444603323936462, Loss G: 4.4441447257995605\n",
      "Epoch [10/30], Batch 30/100, Loss D: 0.3714154362678528, Loss G: 1.8028007745742798\n",
      "Epoch [10/30], Batch 45/100, Loss D: 0.18983319401741028, Loss G: 2.068481922149658\n",
      "Epoch [10/30], Batch 60/100, Loss D: 0.13540831208229065, Loss G: 2.522191286087036\n",
      "Epoch [10/30], Batch 75/100, Loss D: 0.08313842862844467, Loss G: 4.30377721786499\n",
      "Epoch [10/30], Batch 90/100, Loss D: 0.3079710006713867, Loss G: 2.9173221588134766\n",
      "Epoch [11/30], Batch 15/100, Loss D: 0.662209689617157, Loss G: 6.833765506744385\n",
      "Epoch [11/30], Batch 30/100, Loss D: 0.19040367007255554, Loss G: 2.9832522869110107\n",
      "Epoch [11/30], Batch 45/100, Loss D: 0.055702611804008484, Loss G: 3.2246148586273193\n",
      "Epoch [11/30], Batch 60/100, Loss D: 0.36850839853286743, Loss G: 3.760791540145874\n",
      "Epoch [11/30], Batch 75/100, Loss D: 0.3205104172229767, Loss G: 1.7952135801315308\n",
      "Epoch [11/30], Batch 90/100, Loss D: 0.1679801642894745, Loss G: 4.57196569442749\n",
      "Epoch [12/30], Batch 15/100, Loss D: 0.16256071627140045, Loss G: 3.163663148880005\n",
      "Epoch [12/30], Batch 30/100, Loss D: 0.10232037305831909, Loss G: 2.846486806869507\n",
      "Epoch [12/30], Batch 45/100, Loss D: 0.11826331913471222, Loss G: 3.5767433643341064\n",
      "Epoch [12/30], Batch 60/100, Loss D: 0.06180368363857269, Loss G: 3.723844528198242\n",
      "Epoch [12/30], Batch 75/100, Loss D: 0.7676233053207397, Loss G: 10.992964744567871\n",
      "Epoch [12/30], Batch 90/100, Loss D: 0.13457776606082916, Loss G: 4.7762885093688965\n",
      "Epoch [13/30], Batch 15/100, Loss D: 0.08716119825839996, Loss G: 4.9034104347229\n",
      "Epoch [13/30], Batch 30/100, Loss D: 0.29303452372550964, Loss G: 5.067140579223633\n",
      "Epoch [13/30], Batch 45/100, Loss D: 0.17890487611293793, Loss G: 1.9501084089279175\n",
      "Epoch [13/30], Batch 60/100, Loss D: 0.11177396774291992, Loss G: 3.736894369125366\n",
      "Epoch [13/30], Batch 75/100, Loss D: 0.3245603144168854, Loss G: 4.659265041351318\n",
      "Epoch [13/30], Batch 90/100, Loss D: 0.2036573439836502, Loss G: 4.2382636070251465\n",
      "Epoch [14/30], Batch 15/100, Loss D: 0.09268417209386826, Loss G: 2.8130300045013428\n",
      "Epoch [14/30], Batch 30/100, Loss D: 0.4316829741001129, Loss G: 6.489624977111816\n",
      "Epoch [14/30], Batch 45/100, Loss D: 0.09630299359560013, Loss G: 2.981884002685547\n",
      "Epoch [14/30], Batch 60/100, Loss D: 0.13815248012542725, Loss G: 2.6449739933013916\n",
      "Epoch [14/30], Batch 75/100, Loss D: 0.07713326811790466, Loss G: 2.8202412128448486\n",
      "Epoch [14/30], Batch 90/100, Loss D: 0.07785999774932861, Loss G: 4.383975505828857\n",
      "Epoch [15/30], Batch 15/100, Loss D: 0.05038762465119362, Loss G: 4.482086181640625\n",
      "Epoch [15/30], Batch 30/100, Loss D: 0.07427624613046646, Loss G: 3.439105272293091\n",
      "Epoch [15/30], Batch 45/100, Loss D: 0.09490169584751129, Loss G: 2.5512521266937256\n",
      "Epoch [15/30], Batch 60/100, Loss D: 0.12727057933807373, Loss G: 2.4522953033447266\n",
      "Epoch [15/30], Batch 75/100, Loss D: 0.0623101107776165, Loss G: 4.307666301727295\n",
      "Epoch [15/30], Batch 90/100, Loss D: 0.05020572990179062, Loss G: 3.2620513439178467\n",
      "Epoch [16/30], Batch 15/100, Loss D: 0.3978410065174103, Loss G: 9.521185874938965\n",
      "Epoch [16/30], Batch 30/100, Loss D: 0.11820338666439056, Loss G: 4.614869117736816\n",
      "Epoch [16/30], Batch 45/100, Loss D: 0.04436981678009033, Loss G: 4.123839378356934\n",
      "Epoch [16/30], Batch 60/100, Loss D: 0.03695055469870567, Loss G: 3.515749454498291\n",
      "Epoch [16/30], Batch 75/100, Loss D: 0.03836207836866379, Loss G: 3.997021436691284\n",
      "Epoch [16/30], Batch 90/100, Loss D: 0.03274059668183327, Loss G: 4.118481159210205\n",
      "Epoch [17/30], Batch 15/100, Loss D: 0.01904488541185856, Loss G: 5.455336570739746\n",
      "Epoch [17/30], Batch 30/100, Loss D: 0.10522346943616867, Loss G: 2.013293981552124\n",
      "Epoch [17/30], Batch 45/100, Loss D: 0.017430637031793594, Loss G: 4.167747974395752\n",
      "Epoch [17/30], Batch 60/100, Loss D: 0.244290292263031, Loss G: 7.817892551422119\n",
      "Epoch [17/30], Batch 75/100, Loss D: 0.2614916265010834, Loss G: 3.7481040954589844\n",
      "Epoch [17/30], Batch 90/100, Loss D: 0.28540855646133423, Loss G: 6.811820983886719\n",
      "Epoch [18/30], Batch 15/100, Loss D: 0.06423180550336838, Loss G: 4.465310573577881\n",
      "Epoch [18/30], Batch 30/100, Loss D: 0.03341914713382721, Loss G: 3.6701695919036865\n",
      "Epoch [18/30], Batch 45/100, Loss D: 0.05654338374733925, Loss G: 5.600308895111084\n",
      "Epoch [18/30], Batch 60/100, Loss D: 0.10227104276418686, Loss G: 3.191777229309082\n",
      "Epoch [18/30], Batch 75/100, Loss D: 0.08425314724445343, Loss G: 5.327434062957764\n",
      "Epoch [18/30], Batch 90/100, Loss D: 0.03456098586320877, Loss G: 3.367715835571289\n",
      "Epoch [19/30], Batch 15/100, Loss D: 0.016118109226226807, Loss G: 4.114421367645264\n",
      "Epoch [19/30], Batch 30/100, Loss D: 0.09010378271341324, Loss G: 5.192633628845215\n",
      "Epoch [19/30], Batch 45/100, Loss D: 0.056092627346515656, Loss G: 3.8858344554901123\n",
      "Epoch [19/30], Batch 60/100, Loss D: 0.20470605790615082, Loss G: 2.029757499694824\n",
      "Epoch [19/30], Batch 75/100, Loss D: 0.20024947822093964, Loss G: 5.449728488922119\n",
      "Epoch [19/30], Batch 90/100, Loss D: 0.030755266547203064, Loss G: 4.733301639556885\n",
      "Epoch [20/30], Batch 15/100, Loss D: 0.2548031806945801, Loss G: 0.6797230839729309\n",
      "Epoch [20/30], Batch 30/100, Loss D: 0.7286202907562256, Loss G: 3.8536813259124756\n",
      "Epoch [20/30], Batch 45/100, Loss D: 0.09146009385585785, Loss G: 2.974618434906006\n",
      "Epoch [20/30], Batch 60/100, Loss D: 0.22664402425289154, Loss G: 4.028225421905518\n",
      "Epoch [20/30], Batch 75/100, Loss D: 0.04237949103116989, Loss G: 5.035224437713623\n",
      "Epoch [20/30], Batch 90/100, Loss D: 0.08132931590080261, Loss G: 3.3790836334228516\n",
      "Epoch [21/30], Batch 15/100, Loss D: 0.12869450449943542, Loss G: 4.219335079193115\n",
      "Epoch [21/30], Batch 30/100, Loss D: 0.49189648032188416, Loss G: 0.30953702330589294\n",
      "Epoch [21/30], Batch 45/100, Loss D: 0.06593194603919983, Loss G: 4.763432502746582\n",
      "Epoch [21/30], Batch 60/100, Loss D: 0.04100542515516281, Loss G: 5.002230167388916\n",
      "Epoch [21/30], Batch 75/100, Loss D: 0.15410150587558746, Loss G: 4.002732753753662\n",
      "Epoch [21/30], Batch 90/100, Loss D: 0.08563121408224106, Loss G: 5.508892059326172\n",
      "Epoch [22/30], Batch 15/100, Loss D: 0.025401847437024117, Loss G: 4.748680114746094\n",
      "Epoch [22/30], Batch 30/100, Loss D: 0.03634941577911377, Loss G: 4.4972453117370605\n",
      "Epoch [22/30], Batch 45/100, Loss D: 0.20085467398166656, Loss G: 3.8238701820373535\n",
      "Epoch [22/30], Batch 60/100, Loss D: 0.16764740645885468, Loss G: 4.462544918060303\n",
      "Epoch [22/30], Batch 75/100, Loss D: 0.19448435306549072, Loss G: 3.5536301136016846\n",
      "Epoch [22/30], Batch 90/100, Loss D: 0.07292085886001587, Loss G: 3.406132936477661\n",
      "Epoch [23/30], Batch 15/100, Loss D: 0.03050835430622101, Loss G: 4.424325942993164\n",
      "Epoch [23/30], Batch 30/100, Loss D: 0.04687684401869774, Loss G: 3.1552295684814453\n",
      "Epoch [23/30], Batch 45/100, Loss D: 0.034126561135053635, Loss G: 4.274936676025391\n",
      "Epoch [23/30], Batch 60/100, Loss D: 0.03192046284675598, Loss G: 4.745065689086914\n",
      "Epoch [23/30], Batch 75/100, Loss D: 0.15466728806495667, Loss G: 5.072403430938721\n",
      "Epoch [23/30], Batch 90/100, Loss D: 0.02471272647380829, Loss G: 5.77914571762085\n",
      "Epoch [24/30], Batch 15/100, Loss D: 0.04650984704494476, Loss G: 4.083553314208984\n",
      "Epoch [24/30], Batch 30/100, Loss D: 0.025349268689751625, Loss G: 6.1126227378845215\n",
      "Epoch [24/30], Batch 45/100, Loss D: 0.010525118559598923, Loss G: 5.619499683380127\n",
      "Epoch [24/30], Batch 60/100, Loss D: 0.09053384512662888, Loss G: 7.1676788330078125\n",
      "Epoch [24/30], Batch 75/100, Loss D: 0.06603525578975677, Loss G: 4.168466567993164\n",
      "Epoch [24/30], Batch 90/100, Loss D: 0.17976821959018707, Loss G: 2.982741117477417\n",
      "Epoch [25/30], Batch 15/100, Loss D: 0.10860227048397064, Loss G: 3.0388729572296143\n",
      "Epoch [25/30], Batch 30/100, Loss D: 0.018618490546941757, Loss G: 6.249539852142334\n",
      "Epoch [25/30], Batch 45/100, Loss D: 0.04813969135284424, Loss G: 4.321699619293213\n",
      "Epoch [25/30], Batch 60/100, Loss D: 0.07463420182466507, Loss G: 3.6240291595458984\n",
      "Epoch [25/30], Batch 75/100, Loss D: 0.016782764345407486, Loss G: 6.990886688232422\n",
      "Epoch [25/30], Batch 90/100, Loss D: 0.13193601369857788, Loss G: 2.731430768966675\n",
      "Epoch [26/30], Batch 15/100, Loss D: 0.05004289746284485, Loss G: 5.338559627532959\n",
      "Epoch [26/30], Batch 30/100, Loss D: 0.015891870483756065, Loss G: 7.1142897605896\n",
      "Epoch [26/30], Batch 45/100, Loss D: 0.12361449003219604, Loss G: 6.200552940368652\n",
      "Epoch [26/30], Batch 60/100, Loss D: 0.18143558502197266, Loss G: 4.140424728393555\n",
      "Epoch [26/30], Batch 75/100, Loss D: 0.08623596280813217, Loss G: 5.484912395477295\n",
      "Epoch [26/30], Batch 90/100, Loss D: 0.07327034324407578, Loss G: 5.180516719818115\n",
      "Epoch [27/30], Batch 15/100, Loss D: 0.03521115332841873, Loss G: 4.403181076049805\n",
      "Epoch [27/30], Batch 30/100, Loss D: 0.017449282109737396, Loss G: 5.295770168304443\n",
      "Epoch [27/30], Batch 45/100, Loss D: 0.1250969022512436, Loss G: 5.367058753967285\n",
      "Epoch [27/30], Batch 60/100, Loss D: 0.03203577175736427, Loss G: 4.319830417633057\n",
      "Epoch [27/30], Batch 75/100, Loss D: 0.051387686282396317, Loss G: 1.6109182834625244\n",
      "Epoch [27/30], Batch 90/100, Loss D: 0.07395057380199432, Loss G: 3.0280933380126953\n",
      "Epoch [28/30], Batch 15/100, Loss D: 0.02574285864830017, Loss G: 5.159738063812256\n",
      "Epoch [28/30], Batch 30/100, Loss D: 0.06086867302656174, Loss G: 3.7934257984161377\n",
      "Epoch [28/30], Batch 45/100, Loss D: 0.020826080814003944, Loss G: 3.9245870113372803\n",
      "Epoch [28/30], Batch 60/100, Loss D: 0.0327698290348053, Loss G: 4.954860210418701\n",
      "Epoch [28/30], Batch 75/100, Loss D: 0.01252687443047762, Loss G: 5.33077335357666\n",
      "Epoch [28/30], Batch 90/100, Loss D: 0.054192207753658295, Loss G: 3.0174033641815186\n",
      "Epoch [29/30], Batch 15/100, Loss D: 0.0057061114348471165, Loss G: 6.662701606750488\n",
      "Epoch [29/30], Batch 30/100, Loss D: 0.021479405462741852, Loss G: 5.011116981506348\n",
      "Epoch [29/30], Batch 45/100, Loss D: 0.030918262898921967, Loss G: 5.630136013031006\n",
      "Epoch [29/30], Batch 60/100, Loss D: 0.009332956746220589, Loss G: 5.684131145477295\n",
      "Epoch [29/30], Batch 75/100, Loss D: 0.003610762069001794, Loss G: 8.045852661132812\n",
      "Epoch [29/30], Batch 90/100, Loss D: 0.03903637453913689, Loss G: 5.493579864501953\n",
      "Epoch [30/30], Batch 15/100, Loss D: 0.4617382884025574, Loss G: 5.8802337646484375\n",
      "Epoch [30/30], Batch 30/100, Loss D: 0.20452040433883667, Loss G: 4.5260491371154785\n",
      "Epoch [30/30], Batch 45/100, Loss D: 0.039111535996198654, Loss G: 7.380660533905029\n",
      "Epoch [30/30], Batch 60/100, Loss D: 0.04903537407517433, Loss G: 4.851198673248291\n",
      "Epoch [30/30], Batch 75/100, Loss D: 0.147328719496727, Loss G: 2.0620994567871094\n",
      "Epoch [30/30], Batch 90/100, Loss D: 0.22759149968624115, Loss G: 8.471907615661621\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss_D</td><td>▁▁▁▁▁▁▁▂▆▄▃▃▄▅▃▂▂▂▂▂▂▂▅▆▁▂▁▂▄▃▂█▂▁▂▁▁▁▁▂</td></tr><tr><td>Loss_G</td><td>▄▆▇▇██▇▆▂▁▄▂▆▁▂▄▃▄▄▃▃▃▄█▄▄▃▄▃▃▄▃▃▄▅▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss_D</td><td>0.1178</td></tr><tr><td>Loss_G</td><td>5.03343</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">breezy-frog-21</strong> at: <a href='https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb' target=\"_blank\">https://wandb.ai/hj6hki123/gan_training/runs/ffak0xsb</a><br/>Synced 7 W&B file(s), 160 media file(s), 3 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230524_131555-ffak0xsb\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練GAN\n",
    "for epoch in range(P.num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(P.device)\n",
    "\n",
    "        #隨機雜訊->生成器->假圖像\n",
    "        z = torch.randn(batch_size, P.z_dim, 1, 1).to(P.device)\n",
    "        fake_images = generator(z)\n",
    "\n",
    "        # 訓練鑑別器_真實圖像\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(real_images)\n",
    "        real_loss = criterion(real_output,  torch.ones_like(real_output))\n",
    "\n",
    "        # 訓練鑑別器_假圖像\n",
    "        fake_output = discriminator(fake_images.detach())\n",
    "        fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "\n",
    "\n",
    "        # 總鑑別器損失\n",
    "        loss_D = (real_loss + fake_loss) / 2\n",
    "        # 更新鑑別器的權重\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "        # 雜訊->[生成器]->假圖像->[鑑別器]-> 機率\n",
    "        # 訓練生成器\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_output = discriminator(fake_images)\n",
    "        loss_G = criterion(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "        # 更新生成器的權重\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "\n",
    "        wandb.log({\"Loss_D\": loss_D.item(), \"Loss_G\": loss_G.item()}, step=step)\n",
    "\n",
    "        # 顯示訓練進度\n",
    "        if (i + 1) % 15 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{P.num_epochs}], Batch {i+1}/{len(dataloader)}, Loss D: {loss_D.item()}, Loss G: {loss_G.item()}\")\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            generated_images = [wandb.Image(image) for image in fake_images]\n",
    "            wandb.log({\"Generated Images\": generated_images}, step=step)\n",
    "\n",
    "\n",
    "        step = (epoch * len(dataloader) + i)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T05:16:51.776090700Z",
     "start_time": "2023-05-24T05:16:02.555435600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
