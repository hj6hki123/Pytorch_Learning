{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:46:39.728553500Z",
     "start_time": "2023-05-18T06:46:39.705678700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:46:39.735534800Z",
     "start_time": "2023-05-18T06:46:39.712574700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:46:39.747056500Z",
     "start_time": "2023-05-18T06:46:39.728553500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 5\n",
    "num_epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:46:39.747056500Z",
     "start_time": "2023-05-18T06:46:39.728553500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(root='./dataset/train_images', transform=transforms)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  #每次訓練數量 = Data set size(*0.8) / Batch size = 800/20 = 40\n",
    "print(len(loader))\n",
    "\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:46:39.747056500Z",
     "start_time": "2023-05-18T06:46:39.728553500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Batch 0/200                       Loss D: 0.7877, loss G: 0.7466\n",
      "Epoch [1/200] Batch 0/200                       Loss D: 0.1129, loss G: 2.3337\n",
      "Epoch [2/200] Batch 0/200                       Loss D: 0.3793, loss G: 1.1988\n",
      "Epoch [3/200] Batch 0/200                       Loss D: 0.5522, loss G: 1.0377\n",
      "Epoch [4/200] Batch 0/200                       Loss D: 0.2842, loss G: 1.6050\n",
      "Epoch [5/200] Batch 0/200                       Loss D: 0.3436, loss G: 1.3541\n",
      "Epoch [6/200] Batch 0/200                       Loss D: 0.6617, loss G: 0.8141\n",
      "Epoch [7/200] Batch 0/200                       Loss D: 0.4109, loss G: 1.1499\n",
      "Epoch [8/200] Batch 0/200                       Loss D: 0.5512, loss G: 0.7726\n",
      "Epoch [9/200] Batch 0/200                       Loss D: 0.6570, loss G: 0.9567\n",
      "Epoch [10/200] Batch 0/200                       Loss D: 0.6718, loss G: 0.7075\n",
      "Epoch [11/200] Batch 0/200                       Loss D: 0.4314, loss G: 1.1554\n",
      "Epoch [12/200] Batch 0/200                       Loss D: 0.7154, loss G: 0.7054\n",
      "Epoch [13/200] Batch 0/200                       Loss D: 0.4533, loss G: 1.2687\n",
      "Epoch [14/200] Batch 0/200                       Loss D: 0.6593, loss G: 0.9775\n",
      "Epoch [15/200] Batch 0/200                       Loss D: 0.7537, loss G: 0.8466\n",
      "Epoch [16/200] Batch 0/200                       Loss D: 0.5030, loss G: 1.4681\n",
      "Epoch [17/200] Batch 0/200                       Loss D: 0.5432, loss G: 0.9371\n",
      "Epoch [18/200] Batch 0/200                       Loss D: 0.5778, loss G: 1.0793\n",
      "Epoch [19/200] Batch 0/200                       Loss D: 0.5390, loss G: 0.8818\n",
      "Epoch [20/200] Batch 0/200                       Loss D: 0.5189, loss G: 1.0893\n",
      "Epoch [21/200] Batch 0/200                       Loss D: 0.3140, loss G: 1.7527\n",
      "Epoch [22/200] Batch 0/200                       Loss D: 0.2203, loss G: 1.4583\n",
      "Epoch [23/200] Batch 0/200                       Loss D: 0.8450, loss G: 0.6386\n",
      "Epoch [24/200] Batch 0/200                       Loss D: 0.6893, loss G: 0.8223\n",
      "Epoch [25/200] Batch 0/200                       Loss D: 0.6633, loss G: 1.3177\n",
      "Epoch [26/200] Batch 0/200                       Loss D: 0.3975, loss G: 1.3005\n",
      "Epoch [27/200] Batch 0/200                       Loss D: 0.3777, loss G: 1.9986\n",
      "Epoch [28/200] Batch 0/200                       Loss D: 0.4769, loss G: 0.7765\n",
      "Epoch [29/200] Batch 0/200                       Loss D: 0.5776, loss G: 1.2564\n",
      "Epoch [30/200] Batch 0/200                       Loss D: 0.4694, loss G: 1.6485\n",
      "Epoch [31/200] Batch 0/200                       Loss D: 0.2854, loss G: 2.0043\n",
      "Epoch [32/200] Batch 0/200                       Loss D: 0.5254, loss G: 1.7020\n",
      "Epoch [33/200] Batch 0/200                       Loss D: 0.4690, loss G: 1.4947\n",
      "Epoch [34/200] Batch 0/200                       Loss D: 0.3833, loss G: 1.5411\n",
      "Epoch [35/200] Batch 0/200                       Loss D: 0.2863, loss G: 1.7749\n",
      "Epoch [36/200] Batch 0/200                       Loss D: 0.3592, loss G: 1.4620\n",
      "Epoch [37/200] Batch 0/200                       Loss D: 0.2253, loss G: 1.8354\n",
      "Epoch [38/200] Batch 0/200                       Loss D: 0.4579, loss G: 1.1535\n",
      "Epoch [39/200] Batch 0/200                       Loss D: 0.7149, loss G: 1.0890\n",
      "Epoch [40/200] Batch 0/200                       Loss D: 0.4275, loss G: 1.0925\n",
      "Epoch [41/200] Batch 0/200                       Loss D: 0.6944, loss G: 0.7005\n",
      "Epoch [42/200] Batch 0/200                       Loss D: 0.3723, loss G: 1.0891\n",
      "Epoch [43/200] Batch 0/200                       Loss D: 0.5712, loss G: 1.3950\n",
      "Epoch [44/200] Batch 0/200                       Loss D: 0.2990, loss G: 1.8995\n",
      "Epoch [45/200] Batch 0/200                       Loss D: 0.5912, loss G: 0.6075\n",
      "Epoch [46/200] Batch 0/200                       Loss D: 0.5362, loss G: 1.0285\n",
      "Epoch [47/200] Batch 0/200                       Loss D: 0.4902, loss G: 1.6027\n",
      "Epoch [48/200] Batch 0/200                       Loss D: 0.5003, loss G: 0.7890\n",
      "Epoch [49/200] Batch 0/200                       Loss D: 0.4055, loss G: 1.3440\n",
      "Epoch [50/200] Batch 0/200                       Loss D: 0.4179, loss G: 1.5535\n",
      "Epoch [51/200] Batch 0/200                       Loss D: 0.5441, loss G: 1.1776\n",
      "Epoch [52/200] Batch 0/200                       Loss D: 0.3599, loss G: 1.8481\n",
      "Epoch [53/200] Batch 0/200                       Loss D: 0.3943, loss G: 1.2597\n",
      "Epoch [54/200] Batch 0/200                       Loss D: 0.4048, loss G: 1.4015\n",
      "Epoch [55/200] Batch 0/200                       Loss D: 0.4772, loss G: 1.5887\n",
      "Epoch [56/200] Batch 0/200                       Loss D: 0.3432, loss G: 1.8326\n",
      "Epoch [57/200] Batch 0/200                       Loss D: 0.4096, loss G: 1.5966\n",
      "Epoch [58/200] Batch 0/200                       Loss D: 0.5391, loss G: 1.2497\n",
      "Epoch [59/200] Batch 0/200                       Loss D: 0.3346, loss G: 1.8901\n",
      "Epoch [60/200] Batch 0/200                       Loss D: 0.4985, loss G: 1.9493\n",
      "Epoch [61/200] Batch 0/200                       Loss D: 0.5220, loss G: 1.4050\n",
      "Epoch [62/200] Batch 0/200                       Loss D: 0.4837, loss G: 1.3355\n",
      "Epoch [63/200] Batch 0/200                       Loss D: 0.6405, loss G: 1.6680\n",
      "Epoch [64/200] Batch 0/200                       Loss D: 0.4342, loss G: 1.3153\n",
      "Epoch [65/200] Batch 0/200                       Loss D: 0.1976, loss G: 2.0303\n",
      "Epoch [66/200] Batch 0/200                       Loss D: 0.5846, loss G: 1.2187\n",
      "Epoch [67/200] Batch 0/200                       Loss D: 0.6452, loss G: 0.9760\n",
      "Epoch [68/200] Batch 0/200                       Loss D: 0.4733, loss G: 1.2277\n",
      "Epoch [69/200] Batch 0/200                       Loss D: 0.4350, loss G: 1.2426\n",
      "Epoch [70/200] Batch 0/200                       Loss D: 0.1717, loss G: 2.2185\n",
      "Epoch [71/200] Batch 0/200                       Loss D: 0.4233, loss G: 1.5645\n",
      "Epoch [72/200] Batch 0/200                       Loss D: 0.2850, loss G: 1.7859\n",
      "Epoch [73/200] Batch 0/200                       Loss D: 0.4623, loss G: 1.4350\n",
      "Epoch [74/200] Batch 0/200                       Loss D: 0.3697, loss G: 1.5067\n",
      "Epoch [75/200] Batch 0/200                       Loss D: 0.4226, loss G: 1.7502\n",
      "Epoch [76/200] Batch 0/200                       Loss D: 0.3305, loss G: 2.6450\n",
      "Epoch [77/200] Batch 0/200                       Loss D: 0.3211, loss G: 1.1092\n",
      "Epoch [78/200] Batch 0/200                       Loss D: 0.4309, loss G: 1.5822\n",
      "Epoch [79/200] Batch 0/200                       Loss D: 0.8387, loss G: 0.7006\n",
      "Epoch [80/200] Batch 0/200                       Loss D: 0.4040, loss G: 1.3327\n",
      "Epoch [81/200] Batch 0/200                       Loss D: 0.5701, loss G: 1.1198\n",
      "Epoch [82/200] Batch 0/200                       Loss D: 0.5357, loss G: 1.3752\n",
      "Epoch [83/200] Batch 0/200                       Loss D: 0.4916, loss G: 1.1657\n",
      "Epoch [84/200] Batch 0/200                       Loss D: 0.8211, loss G: 1.2709\n",
      "Epoch [85/200] Batch 0/200                       Loss D: 0.3796, loss G: 1.4388\n",
      "Epoch [86/200] Batch 0/200                       Loss D: 0.3882, loss G: 1.9446\n",
      "Epoch [87/200] Batch 0/200                       Loss D: 1.1586, loss G: 1.1014\n",
      "Epoch [88/200] Batch 0/200                       Loss D: 0.5240, loss G: 1.1214\n",
      "Epoch [89/200] Batch 0/200                       Loss D: 0.3584, loss G: 1.5957\n",
      "Epoch [90/200] Batch 0/200                       Loss D: 0.3107, loss G: 2.1808\n",
      "Epoch [91/200] Batch 0/200                       Loss D: 0.4540, loss G: 1.7776\n",
      "Epoch [92/200] Batch 0/200                       Loss D: 0.5132, loss G: 1.7422\n",
      "Epoch [93/200] Batch 0/200                       Loss D: 0.2708, loss G: 1.7755\n",
      "Epoch [94/200] Batch 0/200                       Loss D: 0.3529, loss G: 1.8660\n",
      "Epoch [95/200] Batch 0/200                       Loss D: 0.8837, loss G: 0.3699\n",
      "Epoch [96/200] Batch 0/200                       Loss D: 0.3525, loss G: 1.9365\n",
      "Epoch [97/200] Batch 0/200                       Loss D: 0.5508, loss G: 3.4100\n",
      "Epoch [98/200] Batch 0/200                       Loss D: 0.2633, loss G: 1.8852\n",
      "Epoch [99/200] Batch 0/200                       Loss D: 0.2219, loss G: 1.9425\n",
      "Epoch [100/200] Batch 0/200                       Loss D: 0.6566, loss G: 0.8373\n",
      "Epoch [101/200] Batch 0/200                       Loss D: 0.2823, loss G: 2.6576\n",
      "Epoch [102/200] Batch 0/200                       Loss D: 0.3721, loss G: 1.6393\n",
      "Epoch [103/200] Batch 0/200                       Loss D: 0.4081, loss G: 1.3995\n",
      "Epoch [104/200] Batch 0/200                       Loss D: 0.3518, loss G: 1.7089\n",
      "Epoch [105/200] Batch 0/200                       Loss D: 0.6228, loss G: 1.7470\n",
      "Epoch [106/200] Batch 0/200                       Loss D: 0.3133, loss G: 1.7555\n",
      "Epoch [107/200] Batch 0/200                       Loss D: 0.3684, loss G: 1.6711\n",
      "Epoch [108/200] Batch 0/200                       Loss D: 0.5394, loss G: 1.4212\n",
      "Epoch [109/200] Batch 0/200                       Loss D: 0.3391, loss G: 1.4819\n",
      "Epoch [110/200] Batch 0/200                       Loss D: 0.7864, loss G: 1.0484\n",
      "Epoch [111/200] Batch 0/200                       Loss D: 0.3513, loss G: 2.4559\n",
      "Epoch [112/200] Batch 0/200                       Loss D: 0.6557, loss G: 1.4199\n",
      "Epoch [113/200] Batch 0/200                       Loss D: 0.9889, loss G: 1.2222\n",
      "Epoch [114/200] Batch 0/200                       Loss D: 0.9127, loss G: 0.7868\n",
      "Epoch [115/200] Batch 0/200                       Loss D: 0.5118, loss G: 1.6829\n",
      "Epoch [116/200] Batch 0/200                       Loss D: 0.5071, loss G: 1.3366\n",
      "Epoch [117/200] Batch 0/200                       Loss D: 0.4677, loss G: 1.3840\n",
      "Epoch [118/200] Batch 0/200                       Loss D: 0.3900, loss G: 1.5323\n",
      "Epoch [119/200] Batch 0/200                       Loss D: 0.3555, loss G: 1.6504\n",
      "Epoch [120/200] Batch 0/200                       Loss D: 1.0422, loss G: 0.5479\n",
      "Epoch [121/200] Batch 0/200                       Loss D: 0.5475, loss G: 1.2070\n",
      "Epoch [122/200] Batch 0/200                       Loss D: 0.4922, loss G: 1.3160\n",
      "Epoch [123/200] Batch 0/200                       Loss D: 0.6594, loss G: 1.3945\n",
      "Epoch [124/200] Batch 0/200                       Loss D: 0.8300, loss G: 1.5128\n",
      "Epoch [125/200] Batch 0/200                       Loss D: 0.2424, loss G: 1.6887\n",
      "Epoch [126/200] Batch 0/200                       Loss D: 0.6426, loss G: 2.7381\n",
      "Epoch [127/200] Batch 0/200                       Loss D: 0.3994, loss G: 1.6988\n",
      "Epoch [128/200] Batch 0/200                       Loss D: 0.7878, loss G: 1.2642\n",
      "Epoch [129/200] Batch 0/200                       Loss D: 0.5429, loss G: 1.2655\n",
      "Epoch [130/200] Batch 0/200                       Loss D: 0.6729, loss G: 0.6511\n",
      "Epoch [131/200] Batch 0/200                       Loss D: 0.5809, loss G: 1.8826\n",
      "Epoch [132/200] Batch 0/200                       Loss D: 0.8482, loss G: 1.2336\n",
      "Epoch [133/200] Batch 0/200                       Loss D: 0.1406, loss G: 2.4988\n",
      "Epoch [134/200] Batch 0/200                       Loss D: 0.3753, loss G: 1.8887\n",
      "Epoch [135/200] Batch 0/200                       Loss D: 0.3376, loss G: 1.4439\n",
      "Epoch [136/200] Batch 0/200                       Loss D: 0.2955, loss G: 1.8441\n",
      "Epoch [137/200] Batch 0/200                       Loss D: 0.7213, loss G: 1.2884\n",
      "Epoch [138/200] Batch 0/200                       Loss D: 0.7192, loss G: 1.2599\n",
      "Epoch [139/200] Batch 0/200                       Loss D: 0.4987, loss G: 1.5733\n",
      "Epoch [140/200] Batch 0/200                       Loss D: 0.6015, loss G: 1.3478\n",
      "Epoch [141/200] Batch 0/200                       Loss D: 0.5149, loss G: 1.4480\n",
      "Epoch [142/200] Batch 0/200                       Loss D: 0.2399, loss G: 2.0374\n",
      "Epoch [143/200] Batch 0/200                       Loss D: 0.4550, loss G: 1.5020\n",
      "Epoch [144/200] Batch 0/200                       Loss D: 0.5308, loss G: 1.6397\n",
      "Epoch [145/200] Batch 0/200                       Loss D: 0.1780, loss G: 2.2591\n",
      "Epoch [146/200] Batch 0/200                       Loss D: 0.4452, loss G: 1.3355\n",
      "Epoch [147/200] Batch 0/200                       Loss D: 0.5503, loss G: 1.5825\n",
      "Epoch [148/200] Batch 0/200                       Loss D: 0.5993, loss G: 1.7260\n",
      "Epoch [149/200] Batch 0/200                       Loss D: 0.6194, loss G: 1.1357\n",
      "Epoch [150/200] Batch 0/200                       Loss D: 0.1991, loss G: 2.4368\n",
      "Epoch [151/200] Batch 0/200                       Loss D: 0.3796, loss G: 2.1411\n",
      "Epoch [152/200] Batch 0/200                       Loss D: 0.2603, loss G: 2.2391\n",
      "Epoch [153/200] Batch 0/200                       Loss D: 0.5327, loss G: 2.0025\n",
      "Epoch [154/200] Batch 0/200                       Loss D: 0.5408, loss G: 2.4825\n",
      "Epoch [155/200] Batch 0/200                       Loss D: 0.3910, loss G: 1.7583\n",
      "Epoch [156/200] Batch 0/200                       Loss D: 0.8093, loss G: 1.3673\n",
      "Epoch [157/200] Batch 0/200                       Loss D: 0.8685, loss G: 0.6874\n",
      "Epoch [158/200] Batch 0/200                       Loss D: 0.6598, loss G: 0.9080\n",
      "Epoch [159/200] Batch 0/200                       Loss D: 0.3648, loss G: 1.8285\n",
      "Epoch [160/200] Batch 0/200                       Loss D: 0.3179, loss G: 1.6899\n",
      "Epoch [161/200] Batch 0/200                       Loss D: 0.3923, loss G: 1.0674\n",
      "Epoch [162/200] Batch 0/200                       Loss D: 0.2209, loss G: 2.6446\n",
      "Epoch [163/200] Batch 0/200                       Loss D: 0.2843, loss G: 1.9629\n",
      "Epoch [164/200] Batch 0/200                       Loss D: 0.5717, loss G: 1.3975\n",
      "Epoch [165/200] Batch 0/200                       Loss D: 0.8029, loss G: 0.6714\n",
      "Epoch [166/200] Batch 0/200                       Loss D: 0.3822, loss G: 1.2819\n",
      "Epoch [167/200] Batch 0/200                       Loss D: 0.5778, loss G: 1.0995\n",
      "Epoch [168/200] Batch 0/200                       Loss D: 0.6209, loss G: 2.1253\n",
      "Epoch [169/200] Batch 0/200                       Loss D: 0.9847, loss G: 0.9025\n",
      "Epoch [170/200] Batch 0/200                       Loss D: 0.9236, loss G: 0.6224\n",
      "Epoch [171/200] Batch 0/200                       Loss D: 0.4159, loss G: 2.1305\n",
      "Epoch [172/200] Batch 0/200                       Loss D: 0.6388, loss G: 1.8870\n",
      "Epoch [173/200] Batch 0/200                       Loss D: 0.4567, loss G: 1.6453\n",
      "Epoch [174/200] Batch 0/200                       Loss D: 0.3921, loss G: 1.8420\n",
      "Epoch [175/200] Batch 0/200                       Loss D: 0.3769, loss G: 1.8612\n",
      "Epoch [176/200] Batch 0/200                       Loss D: 0.8928, loss G: 1.1033\n",
      "Epoch [177/200] Batch 0/200                       Loss D: 0.4633, loss G: 1.2463\n",
      "Epoch [178/200] Batch 0/200                       Loss D: 0.4749, loss G: 1.9070\n",
      "Epoch [179/200] Batch 0/200                       Loss D: 0.8071, loss G: 1.0390\n",
      "Epoch [180/200] Batch 0/200                       Loss D: 0.7847, loss G: 1.0671\n",
      "Epoch [181/200] Batch 0/200                       Loss D: 0.5466, loss G: 2.0811\n",
      "Epoch [182/200] Batch 0/200                       Loss D: 0.2297, loss G: 2.9873\n",
      "Epoch [183/200] Batch 0/200                       Loss D: 0.3924, loss G: 1.8344\n",
      "Epoch [184/200] Batch 0/200                       Loss D: 0.5940, loss G: 1.3731\n",
      "Epoch [185/200] Batch 0/200                       Loss D: 0.5822, loss G: 1.6071\n",
      "Epoch [186/200] Batch 0/200                       Loss D: 0.8664, loss G: 2.2228\n",
      "Epoch [187/200] Batch 0/200                       Loss D: 0.4588, loss G: 1.3087\n",
      "Epoch [188/200] Batch 0/200                       Loss D: 0.4360, loss G: 2.4528\n",
      "Epoch [189/200] Batch 0/200                       Loss D: 0.6372, loss G: 1.1015\n",
      "Epoch [190/200] Batch 0/200                       Loss D: 0.4783, loss G: 1.6521\n",
      "Epoch [191/200] Batch 0/200                       Loss D: 0.8691, loss G: 1.3555\n",
      "Epoch [192/200] Batch 0/200                       Loss D: 0.5615, loss G: 1.5317\n",
      "Epoch [193/200] Batch 0/200                       Loss D: 0.3723, loss G: 2.2177\n",
      "Epoch [194/200] Batch 0/200                       Loss D: 1.0640, loss G: 1.8968\n",
      "Epoch [195/200] Batch 0/200                       Loss D: 0.3112, loss G: 2.3963\n",
      "Epoch [196/200] Batch 0/200                       Loss D: 0.9322, loss G: 0.9852\n",
      "Epoch [197/200] Batch 0/200                       Loss D: 0.4453, loss G: 1.5472\n",
      "Epoch [198/200] Batch 0/200                       Loss D: 0.1821, loss G: 2.2445\n",
      "Epoch [199/200] Batch 0/200                       Loss D: 0.3334, loss G: 1.9627\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:50:58.998081400Z",
     "start_time": "2023-05-18T06:46:39.752043200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T06:50:59.000527600Z",
     "start_time": "2023-05-18T06:50:58.999528Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
